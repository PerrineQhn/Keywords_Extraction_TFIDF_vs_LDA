{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modèle TF-IDF"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":624,"status":"ok","timestamp":1714741525747,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"tthGdr-52dQK","outputId":"0c93663e-73ec-4eb2-a66e-6d1525b1036a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/perrine/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /Users/perrine/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /Users/perrine/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import re\n","import nltk\n","import gensim\n","# from google.colab import drive\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","from gensim.parsing.preprocessing import STOPWORDS\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from itertools import combinations, permutations\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"markdown","metadata":{"id":"w0g4Jtv9LWJj"},"source":["## Charger le corpus"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3569,"status":"ok","timestamp":1714754548851,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"xpiKAmnoqPTU","outputId":"a7033f14-9ed9-4d9c-f129-81dbf35be541"},"outputs":[{"name":"stdout","output_type":"stream","text":["Schema:\n","\n","keywords    object\n","sentence    object\n","dtype: object \n","\n","Number of questions,columns= (13014, 2)\n"]}],"source":["# drive.mount('/content/drive')\n","\n","# Choisir le corpus entre \"Wikipedia\" ou \"Stackoverflow\"\n","dataset:str = \"Wikipedia\"\n","\n","# Charger le dataset\n","if dataset == \"Stackoverflow\":\n","  dataframe_tfidf = pd.read_json(\"../data/stackoverflow-data-idf.json\", lines=True)\n","elif  dataset == \"Wikipedia\":\n","  dataframe_tfidf = pd.read_json(\"../data/wikipedia_keywords.json\")\n","\n","# Afficher le schema\n","print(\"Schema:\\n\")\n","print(dataframe_tfidf.dtypes, \"\\n\")\n","print(\"Number of questions,columns=\", dataframe_tfidf.shape)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1714754551304,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"MhQ9cveEXe9K","outputId":"f1b76bae-1d7b-46a7-afe3-470a9d196b5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train set: (7808, 2)\n","Dev set: (2603, 2)\n","Test set: (2603, 2)\n"]}],"source":["# Division du jeu de données en 60% train, 20% dev, 20% test\n","# temp_df : le reste du corpus à part le train_df\n","train_df, temp_df = train_test_split(dataframe_tfidf, test_size=0.4, random_state=42)\n","dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Afficher les dimensions des ensembles pour vérifier\n","print(\"Train set:\", train_df.shape)\n","print(\"Dev set:\", dev_df.shape)\n","print(\"Test set:\", test_df.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prétraitement"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59857,"status":"ok","timestamp":1714754612828,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"RPk9_RfUrFPS","outputId":"b27d640b-3be9-4400-dd43-5de44f3a7e81"},"outputs":[{"name":"stdout","output_type":"stream","text":["Exemple de texte nettoyé :\n","\n","10408                                   refer great artist\n","4099                           histor russian success game\n","5768     italian defeat germani forc north africa end m...\n","8974                                        model principl\n","5350         februari churchil second th british armi base\n","Name: text, dtype: object\n"]}],"source":["# Les fonctions de pré-traitement\n","\n","stemmer = SnowballStemmer('english')\n","\n","# Lemmatisation et stemming\n","def lemmatize_stemming(text:str) -> str:\n","    \"\"\"\n","    Lemmatize and stem the text\n","\n","    Parameters:\n","    text : str : text to be lemmatized and stemmed\n","\n","    Returns:\n","    str : lemmatized and stemmed text\n","    \"\"\"\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='n'))\n","\n","\n","# Nettoyer le texte\n","def pre_process(text:str) -> str:\n","    \"\"\"\n","    Convert to lowercase, remove html tags, remove special characters and numbers\n","\n","    Parameters:\n","    text : str : text to be cleaned\n","\n","    Returns:\n","    text : str : cleaned text\n","    \"\"\"\n","    # Mettre en minuscule\n","    text = text.lower()\n","    # Enlever les balises html\n","    text = re.sub(\"</?.*?>\",\" <> \",text)\n","    # Enlever les caractères spéciaux et numéraux\n","    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n","\n","    return text\n","\n","# Fonction pour récupérer la liste des stopwords\n","def get_stop_words(stop_file_path: str) -> frozenset:\n","    \"\"\"\n","    Load stop words\n","\n","    Parameters:\n","    stop_file_path : str : path to the stop words file\n","\n","    Returns:\n","    stop_set : set : set of stop words\n","    \"\"\"\n","    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n","        stopwords = f.readlines()\n","        stop_set = set(m.strip() for m in stopwords)\n","        return frozenset(stop_set)\n","\n","stopwords = get_stop_words(\"../data/stopwords_json.txt\")\n","\n","# Retirer les stopwords depuis le corpus\n","def stopWords(text: str) -> str:\n","    \"\"\"\n","    Remove stopwords and words with less than 3 characters\n","\n","    Parameters:\n","    text : str : text to be cleaned\n","\n","    Returns:\n","    result : str : cleaned text\n","    \"\"\"\n","    result = \"\"\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in STOPWORDS:\n","            if token not in stopwords:\n","                result += lemmatize_stemming(token) + \" \"\n","    return result\n","\n","def no_below(texts:list, min:int) -> list:\n","\n","    \"\"\"\n","    Remove the words which appear in less than {min} documents\n","\n","    Parameters:\n","    texts : list of documents(str)\n","    min : the seuil\n","\n","    Returns:\n","    filtered_texts : list of documents(str) : the filtered documents\n","    \"\"\"\n","    word_counts = {}\n","    for text in texts:\n","        unique_words = set(text.split())\n","        for word in unique_words:\n","            word_counts[word] = word_counts.get(word, 0) + 1\n","\n","    filtered_texts = []\n","    for text in texts:\n","        filtered_text = ' '.join(word for word in text.split() if word_counts[word] > min)\n","        filtered_texts.append(filtered_text)\n","\n","    return filtered_texts\n","\n","# Appliquer la fonction de nettoyage au texte (pré-traitement du corpus)\n","for df in [train_df, dev_df, test_df]:\n","  if dataset == \"Stackoverflow\":\n","      df['text'] = df['title'] + \" \" + df['body']\n","  else:\n","      df['text'] = df['sentence']\n","  # Le corpus prêt à être analysé\n","  df['text'] = df['text'].apply(pre_process).apply(stopWords)\n","  df['text'] = no_below(df['text'], 20)\n","\n","\n","\n","# Affichage des résultats du nettoyage\n","print(\"Exemple de texte nettoyé :\\n\")\n","print(df['text'][:5])"]},{"cell_type":"markdown","metadata":{"id":"eY8RRTh2wtnm"},"source":["Permet de préparer et de nettoyer du texte pour l'analyse linguistique :\n","\n","1. **Initialisation du stemmer** :\n","   - Création d'un outil (`stemmer`) qui simplifie les mots à leur racine. Cela aide à uniformiser différentes formes du même mot.\n","\n","2. **Fonction de lemmatisation et de stemming (`lemmatize_stemming`)** :\n","   - Cette fonction transforme un mot pour obtenir sa forme de base (lemmatisation) et ensuite réduit encore cette forme à sa racine (stemming).\n","\n","3. **Fonction de nettoyage du texte (`pre_process`)** :\n","   - Transforme tout le texte en minuscules, enlève les balises HTML et les caractères spéciaux. Cela rend le texte plus propre et uniforme pour l'analyse.\n","\n","4. **Chargement des stopwords** :\n","   - Les stopwords sont des mots qui sont souvent retirés du texte avant l'analyse car ils sont très fréquents et portent peu d'information utile (comme \"the\", \"and\"). La fonction `get_stop_words` charge ces mots depuis un fichier pour les utiliser plus tard.\n","\n","5. **Fonction de traitement des stopwords (`stopWords`)** :\n","   - Supprime les mots trop courts ou ceux qui figurent dans la liste des stopwords, applique la lemmatisation et le stemming aux mots restants, puis les assemble en un texte nettoyé.\n","\n","6. **Application du nettoyage au texte des DataFrames** :\n","   - Selon le type de données (par exemple, des données de Stackoverflow), le texte est préparé différemment (combinaison de titre et de corps ou utilisation directe des phrases). Chaque texte est ensuite nettoyé et traité pour enlever les mots superflus et simplifier les mots restants.\n","\n","7. **Affichage des résultats** :\n","   - Affiche les premiers exemples de texte après nettoyage pour montrer l'efficacité des méthodes de préparation utilisées.\n","\n","Ce code est utilisé pour améliorer la qualité du texte pour des analyses plus poussées comme la détection de thèmes ou l'analyse de sentiments."]},{"cell_type":"markdown","metadata":{},"source":["## Création du modèle TF-IDF"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":2006,"status":"ok","timestamp":1714754614828,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"zL1rSEUrrHZl"},"outputs":[],"source":["# Création du vectorisateur et du transformateur TF-IDF\n","vectorizer = CountVectorizer(max_df=0.85, max_features=1000)\n","tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n","\n","# Apprentissage sur l'ensemble train (transformation TF-IDF sur le train)\n","train_vectors = vectorizer.fit_transform(train_df['text'])\n","train_tfidf = tfidf_transformer.fit(train_vectors)\n","\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Transformation des ensembles dev et test\n","dev_vectors = vectorizer.transform(dev_df['text'])\n","test_vectors = vectorizer.transform(test_df['text'])\n","dev_tfidf = tfidf_transformer.transform(dev_vectors)\n","test_tfidf = tfidf_transformer.transform(test_vectors)\n","\n","texts = train_df['text'].tolist() # train_df['text'], dev_df['text'] ou test_df['text']"]},{"cell_type":"markdown","metadata":{"id":"3SGjDPCww6ee"},"source":["Ce bloc de code configure et utilise des outils pour transformer le texte nettoyé en valeurs numériques, facilitant leur analyse statistique :\n","\n","1. **Création du vectorisateur et du transformateur TF-IDF** :\n","   - Un `CountVectorizer` est créé avec des options pour ignorer les mots trop fréquents (plus de 85% des textes) et limiter les mots à considérer à 1000. Cela transforme le texte en une matrice de fréquences de mots.\n","   - Un `TfidfTransformer` est également préparé pour ajuster ces fréquences en utilisant la méthode TF-IDF, qui pondère les mots en fonction de leur importance dans les documents.\n","\n","2. **Application sur l'ensemble d'entraînement** :\n","   - Le vectorisateur est d'abord \"entraîné\" avec le texte de l'ensemble d'entraînement (`train_df['text']`), convertissant le texte en une matrice de fréquences.\n","   - Puis, le transformateur TF-IDF est appliqué à cette matrice pour ajuster les fréquences en fonction de leur importance dans l'ensemble du corpus.\n","\n","3. **Extraction des noms de caractéristiques** :\n","   - Les noms des mots (features) retenus par le vectorisateur sont extraits, permettant de savoir quels mots ont été considérés dans l'analyse.\n","\n","4. **Transformation des ensembles de développement et de test** :\n","   - Les textes des ensembles de développement et de test sont transformés en utilisant le vectorisateur déjà entraîné. Cela assure que les mêmes mots et le même traitement sont utilisés pour tous les ensembles.\n","   - Les matrices de fréquences obtenues sont ensuite transformées en utilisant le transformateur TF-IDF pour obtenir les pondérations finales.\n","\n","En résumé, ce code prépare les données textuelles pour des analyses avancées en convertissant les mots en valeurs numériques selon leur fréquence et importance, rendant le texte prêt pour des tâches telles que la classification ou la modélisation de sujets."]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":4264,"status":"ok","timestamp":1714754619090,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"nfSolRPUxNnU"},"outputs":[],"source":["bigram_measures = nltk.collocations.BigramAssocMeasures()\n","finder = nltk.collocations.BigramCollocationFinder.from_words([word for text in texts for word in text.split()])\n","finder.apply_freq_filter(10)\n","bigram_score = finder.score_ngrams(bigram_measures.pmi)\n","\n","bigrams = {}\n","for bigram, score in bigram_score:\n","    bigram = '_'.join(bigram)\n","    bigrams[bigram] = score\n","\n","# bigrams"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":7035,"status":"ok","timestamp":1714754626122,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"xQ7ZkRFPxPBE"},"outputs":[],"source":["trigram_measures = nltk.collocations.TrigramAssocMeasures()\n","finder = nltk.collocations.TrigramCollocationFinder.from_words([word for text in texts for word in text.split()])\n","finder.apply_freq_filter(25)\n","trigram_score = finder.score_ngrams(trigram_measures.pmi)\n","\n","\n","trigrams = {}\n","for trigram, score in trigram_score:\n","    trigram = '_'.join(trigram)\n","    trigrams[trigram] = score\n","\n","# trigrams"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714754626123,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"BIH4gmEErOlb"},"outputs":[],"source":["# Obtenir le texte des colonnes\n","def get_text_columns(dataset: str, dataframe: str) -> list:\n","  \"\"\"\n","  Function to get the text columns of the dataset\n","\n","  Parameters:\n","  dataset: str: the dataset name\n","  dataframe: DataFrame: the dataset\n","\n","  Returns:\n","  list: list of text columns\n","    sentences: list of sentences\n","    tags: list of tags\n","  \"\"\"\n","  if dataset == \"Stackoverflow\":\n","    titles = dataframe['title'].tolist()\n","    bodys = dataframe['body'].tolist()\n","    tags = [ ' '.join([lemmatize_stemming(word) for word in word_tokenize(tag.replace('|', ' ').lower())]) for tag in dataframe['tags'].tolist()]\n","    return titles, bodys, tags\n","  elif dataset == \"Wikipedia\":\n","    sentences = dataframe['sentence'].tolist()\n","    tags = [[lemmatize_stemming(keyword.lower()) for keyword in keyword_list] for keyword_list in dataframe['keywords']]\n","    return sentences, tags\n","\n","def sort_cooc(cooc_matrix) -> list:\n","    \"\"\"\n","    Function to sort the co-occurrence matrix\n","\n","    Parameters:\n","    cooc_matrix: the co-occurrence matrix\n","\n","    Returns:\n","    list: sorted co-occurrence matrix\n","    \"\"\"\n","    tuples_cooc = zip(cooc_matrix.col, cooc_matrix.data)\n","    return sorted(tuples_cooc, key=lambda x: (x[1], x[0]), reverse=True)\n","\n","def extract_keywords(feature_names: list, sorted_items: list, topn=10) -> dict:\n","    \"\"\"\n","    Extract top N keywords from sorted co-occurrence matrix items.\n","\n","    Parameters:\n","    feature_names: list: list of feature names\n","    sorted_items: list: sorted items\n","    topn: int: number of keywords to extract\n","\n","    Returns:\n","    dict: dictionary of keywords\n","    \"\"\"\n","    sorted_items = sorted_items[:topn]\n","    keywords = {feature_names[idx]: round(score, 3) for idx, score in sorted_items}\n","    return keywords\n","\n","def extract_keywords_from_doc(tfidf_vector, doc_index, vectorizer, transformer, topn=10) -> dict:\n","    \"\"\"\n","    Function to extract keywords from a document\n","\n","    Parameters:\n","    tfidf_vector: the tfidf vector\n","    doc_index: the document index\n","    vectorizer: the vectorizer\n","    transformer: the transformer\n","    topn: the number of keywords to extract\n","\n","    Returns:\n","    dict: dictionary of keywords\n","    \"\"\"\n","    row_tfidf_vector = transformer.transform(tfidf_vector[doc_index])\n","    sorted_items = sort_cooc(row_tfidf_vector.tocoo())\n","    keywords = extract_keywords(vectorizer.get_feature_names_out(), sorted_items, topn)\n","    return keywords\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7MIKZfYFxKjI"},"source":["1. **Extraction des textes et des tags** :\n","   - La fonction `get_text_columns` récupère différents types de contenus textuels (titres, corps de texte, tags) selon le type de dataset (par exemple, Stackoverflow ou Wikipedia). Les tags sont nettoyés et simplifiés en utilisant des fonctions de lemmatisation et de stemming pour uniformiser les termes.\n","\n","2. **Tri d'une matrice de cooccurrence** :\n","   - La fonction `sort_cooc` trie les éléments d'une matrice de cooccurrence, qui sont des paires index-valeur indiquant la fréquence ou l'importance d'un mot. Le tri se fait de manière à placer les éléments les plus significatifs en premier, basé sur leur score.\n","\n","3. **Extraction des mots-clés** :\n","   - La fonction `extract_keywords` sélectionne les principaux mots-clés à partir des éléments triés de la matrice de cooccurrence, limitant le nombre de mots-clés à un nombre spécifié (par exemple, les 10 mots les plus pertinents).\n","\n","4. **Extraction de mots-clés d'un document spécifique** :\n","   - `extract_keywords_from_doc` applique les méthodes précédentes à un document spécifique du DataFrame. Elle transforme d'abord le texte en valeurs TF-IDF, trie les résultats, puis extrait et renvoie les mots-clés les plus pertinents pour ce document.\n","\n","Ces fonctions, ensemble, facilitent l'analyse textuelle en permettant d'extraire et de quantifier l'importance des mots dans des textes."]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2032,"status":"ok","timestamp":1714754628152,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"9tHCL3jorV-R","outputId":"b6af66c9-e637-40c1-d43d-c4a6919879fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["=====Sentences:=====\n","In May, Churchill was still generally unpopular with many Conservatives and probably most of the Labour Party.\n","\n","=====Tags:=====\n","['labour parti']\n","\n","=====Keywords:=====\n","labour 0.534\n","conserv 0.496\n","parti 0.417\n","churchil 0.41\n","general 0.357\n"]}],"source":["def print_keywords(idx: int, keywords: dict, columns: list, dataset: str) -> None:\n","    \"\"\"\n","    Function to print the keywords extracted from a document\n","\n","    Parameters:\n","    idx: int: the index of the document\n","    keywords: dict: the keywords extracted\n","    columns: list: the columns of the dataset\n","    dataset: str: the dataset name\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    if dataset == \"Stackoverflow\" :\n","        titles, bodys, tags = columns\n","        print(\"=====Title:=====\")\n","        print(titles[idx])\n","        print(\"\\n=====Text:=====\")\n","        print(bodys[idx])\n","        print(\"\\n=====Tags:=====\")\n","        print(tags[idx])\n","        print(\"\\n=====Keywords:=====\")\n","        for k in keywords:\n","            print(k, keywords[k])\n","    if dataset == \"Wikipedia\" :\n","        sentences, tags = columns\n","        print(\"=====Sentences:=====\")\n","        print(sentences[idx])\n","        print(\"\\n=====Tags:=====\")\n","        print(tags[idx])\n","        print(\"\\n=====Keywords:=====\")\n","        for k in keywords:\n","            print(k, keywords[k])\n","\n","index = 2\n","data_df = train_df # train_df, dev_df ou test_df\n","columns = get_text_columns(dataset, data_df)\n","keywords_and_scores = extract_keywords_from_doc(train_vectors, index, vectorizer, tfidf_transformer, 10)\n","print_keywords(index, keywords_and_scores, columns, dataset)"]},{"cell_type":"markdown","metadata":{"id":"yBDgfzv8yA5V"},"source":["1. **Fonction `print_keywords`** :\n","   - Cette fonction affiche les informations pertinentes d'un document spécifique, comme son titre, son texte, ses tags associés, et les mots-clés extraits. Elle adapte son affichage selon que les données proviennent de Stackoverflow ou de Wikipedia, soulignant la flexibilité de la fonction à différents formats de données.\n","   - Pour chaque mot-clé extrait, la fonction affiche également son score TF-IDF, ce qui indique l'importance du mot dans le document par rapport au corpus total.\n","\n","2. **Utilisation des fonctions** :\n","   - Un index spécifique est choisi pour identifier le document à analyser. Ce choix d'index permet de cibler précisément quel document doit être traité.\n","   - La fonction `get_text_columns` extrait les données textuelles nécessaires (titres, corps, tags) du DataFrame et les prépare pour l'analyse.\n","   - `extract_keywords_from_doc` applique le traitement TF-IDF au document spécifié, trie les mots en fonction de leur importance, et extrait les mots-clés les plus pertinents.\n","   - `print_keywords` est ensuite appelée pour afficher les résultats, permettant de visualiser les mots-clés les plus significatifs pour le document sélectionné, ainsi que le contenu associé pour contexte.\n","\n","3. **Processus détaillé** :\n","   - Après avoir sélectionné le document via un index, le code récupère les données nécessaires à partir du DataFrame en utilisant une fonction dédiée.\n","   - Il transforme ensuite ces données en vecteurs TF-IDF pour évaluer l'importance de chaque mot.\n","   - Les mots-clés sont identifiés en sélectionnant les termes avec les plus hauts scores TF-IDF, et ces mots-clés sont affichés avec leurs scores respectifs pour indiquer clairement leur pertinence.\n","\n","Ces étapes permettent non seulement d'extraire des informations clés des documents textuels, mais aussi de les présenter de manière claire et organisée, facilitant l'analyse du contenu et la compréhension des sujets principaux des documents."]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714754628152,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"EpE5brplxWrJ"},"outputs":[],"source":["# Verify if the combination of keywords and permutation is present in bigrams and trigrams dictionary. If present, add the bigram/trigam to the keyword\n","def verification_bigram_trigram(keywords: dict, bigrams: dict, trigrams: dict) -> dict:\n","    \"\"\"\n","    Verify if the combination of keywords and permutation is present in bigrams and trigrams dictionary. If present, add the bigram/trigam to the keyword\n","\n","    Parameters:\n","    keyword : dict : dictionary containing keywords and their scores\n","    bigrams : dict : dictionary containing bigrams and their scores\n","    trigrams : dict : dictionary containing trigrams and their scores\n","\n","    Returns:\n","    dict : updated dictionary containing keywords and their scores\n","    \"\"\"\n","    updated_keywords = []\n","    for keyword_dict in keywords:  # Utiliser un nom différent pour l'itérateur\n","        updated_keyword_dict = keyword_dict.copy()  # Créer une copie du dictionnaire original\n","        keyword_keys = keyword_dict.keys()  # Utiliser un nom différent pour éviter la confusion\n","        bigram_set = set(bigrams.keys())  # Obtenir les clés du dictionnaire de bigrammes\n","        trigram_set = set(trigrams.keys())  # Obtenir les clés du dictionnaire de trigrammes\n","\n","        combined_keywords = set()\n","        for k in range(1, min(4, len(keyword_keys) + 1)):  # Utiliser min pour ne pas dépasser 3 tokens\n","            for subset in combinations(keyword_keys, k):\n","                for perm in permutations(subset):\n","                    combined_keyword = '_'.join(perm)\n","                    if len(combined_keyword.split()) <= 3:  # Vérifier le nombre de tokens\n","                        combined_keywords.add(combined_keyword)\n","\n","        for combined_keyword in combined_keywords:\n","            if combined_keyword in bigram_set or combined_keyword in trigram_set:\n","                updated_keyword_dict[combined_keyword] = 0.001  # Ajouter une petite valeur pour indiquer qu'il s'agit d'un bigramme/trigramme\n","\n","        updated_keywords.append(updated_keyword_dict)  # Ajouter le dictionnaire mis à jour à la liste\n","\n","    return updated_keywords"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714754628153,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"8NhQg8kUQZvA"},"outputs":[],"source":["def calculate_precision_recall_fscore(true_tags: list, predicted_keywords: list) -> tuple:\n","    \"\"\"\n","    Function to calculate precision, recall and f-score\n","\n","    Parameters:\n","    true_tags: list: the true tags\n","    predicted_keywords: list: the predicted keywords\n","\n","    Returns:\n","    tuple: precision, recall, f-score\n","    \"\"\"\n","    true_set = set(true_tags)  # Conversion des tags en ensemble\n","    predicted_set = set(predicted_keywords)  # Les mots-clés sont déjà un ensemble\n","    tp = len(true_set & predicted_set)\n","    fp = len(predicted_set - true_set)\n","    fn = len(true_set - predicted_set)\n","\n","    precision = tp / (tp + fp) if tp + fp > 0 else 0\n","    recall = tp / (tp + fn) if tp + fn > 0 else 0\n","    f_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n","\n","    return precision, recall, f_score"]},{"cell_type":"markdown","metadata":{"id":"v5ohkLDTRMHa"},"source":["La fonction `calculate_precision_recall_fscore` est conçue pour calculer trois métriques importantes dans l'évaluation des systèmes de récupération d'information ou de classification : la précision, le rappel et la F-mesure (F-score). Voici un détail de chaque étape et de chaque calcul effectué dans la fonction :\n","\n","1. Conversion des Tags en Ensembles :\n","* `true_tags` : Une chaîne de caractères contenant les tags corrects, séparés par des espaces. Ces tags sont convertis en un ensemble (set) pour permettre des opérations ensemblistes.\n","* `predicted_keywords` : Un ensemble de mots-clés prédits. L'utilisation d'un ensemble permet également de réaliser facilement des opérations d'intersection et de différence.\n","\n","2. Calcul des True Positives (tp), False Positives (fp) et False Negatives (fn) :\n","* True Positives (tp) : Le nombre d'éléments qui sont à la fois dans les `true_tags` et dans les `predicted_keywords`. Cela représente le nombre de prédictions correctes.\n","* False Positives (fp) : Le nombre d'éléments qui sont dans `predicted_keywords` mais pas dans `true_tags`. Ces éléments ont été incorrectement prédits comme étant des tags.\n","* False Negatives (fn) : Le nombre d'éléments qui sont dans `true_tags` mais pas dans `predicted_keywords`. Ces éléments sont des tags corrects qui n'ont pas été prédits.\n","\n","3. Calcul de la Précision :\n","* Précision : Cette métrique évalue la qualité des prédictions. Elle est définie comme le ratio des vrais positifs par rapport à la somme des vrais positifs et des faux positifs. Une précision élevée signifie que la majorité des tags prédits sont corrects.\n","* Formule :\n","Précision= tp/(tp+fp)\n","\n","4. Calcul du Rappel :\n","* Rappel : Cette métrique mesure l'exhaustivité des prédictions. Elle est définie comme le ratio des vrais positifs par rapport à la somme des vrais positifs et des faux négatifs. Un rappel élevé signifie que la majorité des tags corrects ont été capturés par les prédictions.\n","* Formule :\n","Rappel = tp/(tp+fn)\n","\n","5. Calcul de la F-mesure (F-score) :\n","* F-score : Cette métrique combine la précision et le rappel dans une seule mesure, utilisant leur moyenne harmonique. Elle est particulièrement utile quand il est important d'équilibrer la précision et le rappel, sans favoriser l'une ou l'autre.\n","* Formule :\n","F1 = 2x((PrécisionxRappel)/(Précision+Rappel))\n","\n","6. Retour des Résultats :\n","La fonction retourne les trois métriques calculées : précision, rappel et F-score.\n","\n","Cette fonction est cruciale pour évaluer les systèmes de classification ou de recommandation où la correspondance exacte entre les éléments prédits et les vérités terrain est essentielle pour mesurer la performance.\n"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":1164,"status":"ok","timestamp":1714754629313,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"Xo90Fy8hxcSA"},"outputs":[],"source":["# Extraire les mots-clés de tous les documents\n","tf_idf_vector = tfidf_transformer.transform(vectorizer.transform(texts))\n","\n","def process_documents(dataframe, tfidf_vector, vectorizer, transformer) -> list:\n","    \"\"\"\n","    Function to process all documents in the dataset\n","\n","    Parameters:\n","    dataframe: DataFrame: the dataset\n","    tfidf_vector: the TF-IDF vector\n","    vectorizer: the vectorizer\n","    transformer: the transformer\n","\n","    Returns:\n","    list: list of all keywords with their scores\n","    \"\"\"\n","    all_keywords = []\n","\n","    for idx in range(dataframe.shape[0]):\n","        keywords = extract_keywords_from_doc(tfidf_vector, idx, vectorizer, transformer)\n","        all_keywords.append(keywords)\n","\n","    return all_keywords"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":51733,"status":"ok","timestamp":1714754681044,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"p-b42XgbrZy1"},"outputs":[],"source":["def matches_keywords(keyword_scores: list, tags: list) -> list:\n","    \"\"\"\n","    Check if the keywords match the tags.\n","\n","    Parameters:\n","    keyword_scores : list : list of dictionaries containing keywords and their scores\n","    tags : list : list of tags\n","\n","    Returns:\n","    list : list of numbers indicating the count of keyword matches per tag list\n","    \"\"\"\n","    pourcentages:list=[]\n","    matches = []\n","    for tag_list, keyword_dict in zip(tags, keyword_scores):\n","        keywords = list(keyword_dict.keys())\n","        # print('keywords: ', keywords, '\\ntag_list: ', tag_list)\n","        tag_ngram = []\n","        # S'il y a des tags de bigrams ou trigrams séparés par espace, on le remplace par '_'\n","        for tag in tag_list:\n","            tag = tag.strip().replace(' ', '_')\n","            # print(tag)\n","            tag_ngram.append(tag)\n","\n","        match = 0\n","        bingo_pourcentage:str = \"\"\n","        for keyword in keywords:\n","            if keyword in tag_ngram:\n","                match += 1\n","        bingo_pourcentage = f\"{round(match / len(tag_ngram), 3) * 100}%\"\n","        pourcentages.append(bingo_pourcentage)\n","        matches.append(match)\n","    return matches, pourcentages\n","\n","if dataset == \"Stackoverflow\":\n","  titles, bodys, tags = get_text_columns(dataset, data_df)\n","  tags = [tag.split(' ') for tag in tags]\n","  train_keywords = process_documents(data_df, train_vectors, vectorizer, tfidf_transformer)\n","  dataframe = pd.DataFrame(zip(titles, bodys, tags), columns=['title', 'body', 'tags'])\n","  train_keywords = verification_bigram_trigram(train_keywords, bigrams, trigrams)\n","  # Ajout des résultats au DataFrame\n","  dataframe['keywords'] = train_keywords\n","  evaluation_scores = [calculate_precision_recall_fscore(tags[i], train_keywords[i]) for i in range(len(tags))]\n","  dataframe['precision'], dataframe['recall'], dataframe['f_score'] = zip(*evaluation_scores)\n","  dataframe['match'], dataframe['match percent'] = matches_keywords(train_keywords, tags)\n","  # print(dataframe[['title', 'body', 'tags', 'keywords', 'match', 'precision', 'recall', 'f_score']].head())\n","  dataframe.to_csv(\"../results/csv/Stackoverflow-data-idf-without-stopwords-match-train_df.csv\", index=False)\n","\n","elif dataset == \"Wikipedia\":\n","  sentences, tags = get_text_columns(dataset, data_df)\n","  train_keywords = process_documents(data_df, train_vectors, vectorizer, tfidf_transformer)\n","  dataframe = pd.DataFrame(zip(sentences, tags), columns=['sentences', 'tags'])\n","  train_keywords = verification_bigram_trigram(train_keywords, bigrams, trigrams)\n","  dataframe['keywords'] = train_keywords\n","  dataframe['match'], dataframe['match percent'] = matches_keywords(train_keywords, tags)\n","  evaluation_scores = [calculate_precision_recall_fscore(tags[i], train_keywords[i]) for i in range(len(tags))]\n","  # print(\"Evaluation scores: \", evaluation_scores)\n","  dataframe['precision'], dataframe['recall'], dataframe['f_score'] = zip(*evaluation_scores)\n","  # print(dataframe[['sentences', 'tags', 'keywords', 'match', 'precision', 'recall', 'f_score']].head())\n","  dataframe.to_csv(\"../results/csv/Wikipedia-data-idf-without-stopwords-match-train_df.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"tB-SfLRKyjPt"},"source":["1. **Transformation des documents en vecteurs TF-IDF** :\n","   - Les documents sont d'abord transformés en vecteurs de mots par le `vectorizer`, puis ces vecteurs sont convertis en représentations TF-IDF grâce au `tfidf_transformer`. Cela permet de préparer les documents pour une analyse plus détaillée basée sur l'importance des mots dans le corpus.\n","\n","2. **Extraction des mots-clés et évaluation des performances** :\n","   - Pour chaque document, les mots-clés sont extraits et évalués en utilisant les fonctions `extract_keywords_from_doc` et `calculate_precision_recall_fscore`. Les mots-clés sont choisis en fonction de leur pertinence calculée par TF-IDF, et les performances sont mesurées par la précision, le rappel, et le score F, qui comparent les mots-clés prédits aux tags réels du document.\n","\n","3. **Calcul du nombre de correspondances entre les tags et les mots-clés** :\n","   - La fonction `matches_keywords` calcule combien de mots-clés extraits correspondent effectivement aux tags des documents, donnant ainsi une mesure de l'exactitude des mots-clés extraits.\n","\n","4. **Préparation et sauvegarde des résultats** :\n","   - Les résultats sont organisés dans un DataFrame de pandas, qui inclut les titres, les textes, les tags originaux, les mots-clés extraits, et les scores de performance pour chaque document.\n","   - Ces données sont ensuite sauvegardées dans un fichier CSV pour une utilisation ultérieure ou une analyse plus approfondie.\n","\n","5. **Affichage des résultats** :\n","   - Les premières lignes du DataFrame sont affichées pour donner un aperçu des mots-clés et des scores associés, permettant de vérifier rapidement l'efficacité de l'extraction et de l'évaluation des mots-clés.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Test"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"GaJHBzikHlIn"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                           sentences  \\\n","0  Laurence had interviewed Sweeney and his crew,...   \n","1  Historically, Russian athletes have been one o...   \n","2  Italian defeats prompted Germany to deploy an ...   \n","3  Genotype evolution can be modeled with the Har...   \n","4  In February 1895, Churchill was commissioned a...   \n","\n","                                                tags  \\\n","0                                 [the great artist]   \n","1             [russian athlet, olympic gam, russian]   \n","2  [deploy an expeditionary forc, rommel, afrika ...   \n","3                          [hardy-weinberg principl]   \n","4  [4th queen's own hussar, british armi, aldershot]   \n","\n","                                            keywords  match  \\\n","0  {'artist': 0.651, 'refer': 0.569, 'great': 0.503}      0   \n","1  {'game': 0.553, 'histor': 0.542, 'success': 0....      1   \n","2  {'forc': 0.549, 'africa': 0.261, 'axi': 0.249,...      2   \n","3                {'principl': 0.782, 'model': 0.623}      0   \n","4  {'februari': 0.44, 'churchil': 0.385, 'base': ...      1   \n","\n","         match percent  precision    recall   f_score  \n","0                 0.0%   0.000000  0.000000  0.000000  \n","1  33.300000000000004%   0.250000  0.333333  0.285714  \n","2                22.2%   0.083333  0.111111  0.095238  \n","3                 0.0%   0.000000  0.000000  0.000000  \n","4  33.300000000000004%   0.000000  0.000000  0.000000  \n"]}],"source":["# Préparation des données de test\n","test_texts = test_df['text'].tolist()\n","test_vectors = vectorizer.transform(test_df['text'])\n","test_tfidf = tfidf_transformer.transform(test_vectors)\n","\n","# Extraction des mots-clés pour les documents de test\n","test_keywords = process_documents(test_df, test_vectors, vectorizer, tfidf_transformer)\n","test_keywords = verification_bigram_trigram(test_keywords, bigrams, trigrams)\n","\n","# Extraction des colonnes de texte et des tags pour les données de test\n","if dataset == \"Stackoverflow\":\n","    test_titles, test_bodys, test_tags = get_text_columns(dataset, test_df)\n","    test_tags = [tag.split(' ') for tag in test_tags]\n","    test_evaluation_scores = [calculate_precision_recall_fscore(test_tags[i], test_keywords[i]) for i in range(len(test_tags))]\n","    test_match_keywords, test_match_percent = matches_keywords(test_keywords, test_tags)\n","    test_dataframe = pd.DataFrame(zip(test_titles, test_bodys, test_tags, test_keywords, test_match_keywords, test_match_percent), columns=['title', 'body', 'tags', 'keywords', 'match', 'match percent'])\n","    test_dataframe['precision'], test_dataframe['recall'], test_dataframe['f_score'] = zip(*test_evaluation_scores)\n","    test_dataframe.to_csv(\"../results/csv/Stackoverflow-data-idf-without-stopwords-match-test_df.csv\", index=False)\n","    print(test_dataframe[['title', 'tags', 'keywords', 'match', 'match percent', 'precision', 'recall', 'f_score']].head())\n","\n","elif dataset == \"Wikipedia\":\n","    test_sentences, test_tags = get_text_columns(dataset, test_df)\n","    test_evaluation_scores = [calculate_precision_recall_fscore(test_tags[i], test_keywords[i]) for i in range(len(test_tags))]\n","    test_match_keywords, test_match_percent = matches_keywords(test_keywords, test_tags)\n","    test_dataframe = pd.DataFrame(zip(test_sentences, test_tags, test_keywords, test_match_keywords, test_match_percent), columns=['sentences', 'tags', 'keywords', 'match', 'match percent'])\n","    test_dataframe['precision'], test_dataframe['recall'], test_dataframe['f_score'] = zip(*test_evaluation_scores)\n","    test_dataframe.to_csv(\"../results/csv/Wikipedia-data-idf-without-stopwords-match-test_df.csv\", index=False)\n","    print(test_dataframe[['sentences', 'tags', 'keywords', 'match', 'match percent', 'precision', 'recall', 'f_score']].head())"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
