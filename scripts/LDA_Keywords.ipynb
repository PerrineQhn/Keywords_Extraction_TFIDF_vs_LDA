{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Modèle LDA"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":4306,"status":"ok","timestamp":1714740412068,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"iiDC49kcQTCh"},"outputs":[],"source":["import pandas as pd\n","import re\n","import gensim\n","import nltk\n","from gensim.parsing.preprocessing import preprocess_string, STOPWORDS\n","from gensim.corpora import Dictionary\n","from gensim.models import LdaModel, CoherenceModel\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split\n","from itertools import combinations, permutations\n","# from google.colab import drive"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1581,"status":"ok","timestamp":1714740413647,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"V-Q1mOCNahSA","outputId":"4462f72b-497e-4466-ee65-b84892a495f0"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /Users/perrine/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /Users/perrine/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to /Users/perrine/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21004,"status":"ok","timestamp":1714740434647,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"y-B2toE2mhkD","outputId":"662bbebc-6ef3-465b-b215-b0e404062893"},"outputs":[],"source":["# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4785,"status":"ok","timestamp":1714750485482,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"0WI-X8zLQVxa","outputId":"1f3b8d3d-95c8-4cb1-fe2d-e84dd0703651"},"outputs":[{"name":"stdout","output_type":"stream","text":["Schema:\n","\n","keywords    object\n","sentence    object\n","dtype: object \n","\n","Number of questions, columns= (13014, 2)\n"]}],"source":["dataset = \"Wikipedia\"\n","\n","# Charger le dataset\n","if dataset == \"Stackoverflow\":\n","  dataframe_lda = pd.read_json(\"../data/stackoverflow-data-idf.json\", lines=True)\n","elif dataset == \"Wikipedia\":\n","  dataframe_lda = pd.read_json(\"../data/wikipedia_keywords.json\")\n","\n","\n","# Afficher le schéma\n","print(\"Schema:\\n\")\n","print(dataframe_lda.dtypes, \"\\n\")\n","print(\"Number of questions, columns=\", dataframe_lda.shape)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1714750488373,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"NJAuxLs46N61","outputId":"23b696c8-cd60-4789-8466-348dfc27d00a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train set: (7808, 2)\n","Dev set: (2603, 2)\n","Test set: (2603, 2)\n"]}],"source":["# Division du jeu de données en 60% train, 20% dev, 20% test\n","train_df, temp_df = train_test_split(dataframe_lda, test_size=0.4, random_state=42)\n","dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Afficher les dimensions des ensembles pour vérifier\n","print(\"Train set:\", train_df.shape)\n","print(\"Dev set:\", dev_df.shape)\n","print(\"Test set:\", test_df.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Prétraitement"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52624,"status":"ok","timestamp":1714750543655,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"kmVRQBQLQZVY","outputId":"5fed435e-668d-43b1-ce64-52d5081c8769"},"outputs":[{"name":"stdout","output_type":"stream","text":["Exemple de texte nettoyé :\n","\n","10408    laurenc interview sweeney crew awar refer airp...\n","4099     histor russian athlet success contend olymp game \n","5768     italian defeat prompt germani deploy expeditio...\n","8974         genotyp evolut model hardi weinberg principl \n","5350     februari churchil commiss second lieuten th qu...\n","Name: text, dtype: object\n"]}],"source":["stemmer = SnowballStemmer('english')\n","\n","def lemmatize_stemming(text:str) -> str:\n","    \"\"\"\n","    Lemmatize and stem the text\n","\n","    Parameters:\n","    text : str : text to be lemmatized and stemmed\n","\n","    Returns:\n","    str : lemmatized and stemmed text\n","    \"\"\"\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='n'))\n","\n","\n","def pre_process(text:str) -> str:\n","    \"\"\"\n","    Convert to lowercase, remove html tags, remove special characters and numbers\n","\n","    Parameters:\n","    text : str : text to be cleaned\n","\n","    Returns:\n","    text : str : cleaned text\n","    \"\"\"\n","    text = text.lower()\n","    text = re.sub(\"</?.*?>\",\" <> \",text)\n","    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n","\n","    return text\n","\n","def get_stop_words(stop_file_path: str) -> frozenset:\n","    \"\"\"\n","    Load stop words\n","\n","    Parameters:\n","    stop_file_path : str : path to the stop words file\n","\n","    Returns:\n","    stop_set : set : set of stop words\n","    \"\"\"\n","    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n","        stopwords = f.readlines()\n","        stop_set = set(m.strip() for m in stopwords)\n","        return frozenset(stop_set)\n","\n","stopwords = get_stop_words(\"../data/stopwords_json.txt\")\n","\n","def stopWords(text: str) -> str:\n","    \"\"\"\n","    Remove stopwords and words with less than 3 characters\n","\n","    Parameters:\n","    text : str : text to be cleaned\n","\n","    Returns:\n","    result : str : cleaned text\n","    \"\"\"\n","    result = \"\"\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in STOPWORDS:\n","            if token not in stopwords:\n","                result += lemmatize_stemming(token) + \" \"\n","    return result\n","\n","# Appliquer la fonction de nettoyage au texte\n","for df in [train_df, dev_df, test_df]:\n","  if dataset == \"Stackoverflow\":\n","      df['text'] = df['title'] + \" \" + df['body']\n","  else:\n","      df['text'] = df['sentence']\n","  df['text'] = df['text'].apply(pre_process).apply(stopWords)\n","\n","\n","# Affichage des résultats du nettoyage\n","print(\"Exemple de texte nettoyé :\\n\")\n","print(df['text'][:5])"]},{"cell_type":"markdown","metadata":{"id":"hKHIeCrnT0W2"},"source":["Ce bloc de code est un processus de prétraitement de texte pour le traitement du langage naturel. Il vise à préparer le texte pour des analyses ultérieures en utilisant des techniques de lemmatisation, de stemming, et un filtrage avancé des stopwords. Voici une explication simplifiée de chaque étape :\n","\n","1. **Initialisation d'un stemmer** :\n","   - Un objet `stemmer` de type `SnowballStemmer` est créé pour l'anglais. Cette étape permet de réduire les mots à leur racine, en éliminant les suffixes qui ne sont pas nécessaires pour la compréhension du sens général du mot.\n","\n","2. **Fonction `lemmatize_stemming`** :\n","   - Cette fonction prend un mot en entrée, le transforme en sa forme de base ou lemmatise en utilisant `WordNetLemmatizer`, ciblant spécifiquement les noms (`pos='n'`). Ensuite, elle applique le stemming pour simplifier encore plus le mot. Cette combinaison aide à normaliser les mots pour une analyse cohérente.\n","\n","3. **Nettoyage du texte avec `pre_process`** :\n","   - Convertit tout le texte en minuscules pour uniformiser la casse.\n","   - Supprime les balises HTML et les caractères spéciaux ou numériques, nettoyant le texte de tout élément qui pourrait perturber l'analyse.\n","\n","4. **Chargement et application des stopwords** :\n","   - La fonction `get_stop_words` charge une liste personnalisée de mots fréquemment utilisés mais peu informatifs (stopwords) à partir d'un fichier. Ces mots sont souvent exclus de l'analyse pour se concentrer sur les termes plus significatifs.\n","\n","5. **Filtrage et prétraitement des mots** :\n","   - La fonction `stopWords` tokenise le texte, exclut les mots trop courts ou les stopwords, et applique les opérations de lemmatisation et de stemming aux mots restants. Le résultat est un texte nettoyé et réduit à des termes essentiels.\n","\n","6. **Application du prétraitement aux données** :\n","   - Selon le dataset (`Stackoverflow` ou autre), les titres et les corps des textes sont combinés en une seule colonne `text`.\n","   - Les fonctions de nettoyage et de prétraitement sont appliquées successivement pour transformer le texte brut en une forme optimisée pour l'analyse.\n","\n","7. **Affichage des résultats** :\n","   - Les premiers exemples de texte prétraité sont affichés, montrant l'efficacité des étapes de nettoyage et de normalisation dans la préparation des données pour des analyses plus poussées, comme la modélisation de sujets.\n","\n","Ces étapes garantissent que le texte est suffisamment simplifié et uniformisé, ce qui est crucial pour obtenir de bons résultats dans des applications de NLP telles que l'analyse de sentiments ou la modélisation de sujets."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"r4F6bk_7Qb79"},"outputs":[{"data":{"text/plain":["{'brest_litovsk': 14.476417130701066,\n"," 'sgt_pepper': 14.33891360695113,\n"," 'alsac_lorrain': 14.21338272486727,\n"," 'ba_maw': 14.21338272486727,\n"," 'href_wiki': 14.201410083201194,\n"," 'puerto_rico': 14.097905507447335,\n"," 'warner_bros': 13.990990303530822,\n"," 'ed_sullivan': 13.202494408724535,\n"," 'hong_kong': 13.097905507447335,\n"," 'benioff_wei': 13.043457723424957,\n"," 'kievan_ru': 12.948038158346275,\n"," 'saint_petersburg': 12.940364230460855,\n"," 'harmon_oscil': 12.862885477783136,\n"," 'meter_ft': 12.710882384338086,\n"," 'kaiser_wilhelm': 12.45042192216812,\n"," 'los_angel': 12.364844367737044,\n"," 'dive_bomber': 12.350886248617206,\n"," 'nobel_prize': 12.268524279059731,\n"," 'herbert_hoover': 12.263423407366865,\n"," 'sq_mi': 12.243756373910788,\n"," 'mu_nu': 12.016985512063766,\n"," 'austro_hungarian': 11.859444235077287,\n"," 'vacuum_tube': 11.83700684595753,\n"," 'alan_ture': 11.789356442361171,\n"," 'labrador_retriev': 11.717971809124405,\n"," 'benito_mussolini': 11.669062208643458,\n"," 'midget_submarin': 11.60570014764603,\n"," 'kennel_club': 11.522802669501704,\n"," 'max_planck': 11.509248523168436,\n"," 'angular_momentum': 11.43202301134261,\n"," 'chamber_deputi': 11.39287643515836,\n"," 'arctic_ocean': 11.380492710702528,\n"," 'pearl_harbor': 11.306492129258753,\n"," 'von_neumann': 11.240835937608077,\n"," 'punch_card': 11.228489617257477,\n"," 'cosmolog_constant': 11.154489035813702,\n"," 'marxist_leninist': 11.043457723424957,\n"," 'fat_man': 10.998369833896419,\n"," 'harri_potter': 10.989518916314156,\n"," 'walk_fame': 10.965455211423683,\n"," 'texa_instrument': 10.92428602244728,\n"," 'wall_street': 10.914174706479992,\n"," 'comintern_pact': 10.901324874003969,\n"," 'lloyd_georg': 10.900499769582916,\n"," 'tripartit_pact': 10.865919537872768,\n"," 'rock_roll': 10.751524368835431,\n"," 'malt_barley': 10.714068315056934,\n"," 'disk_drive': 10.6291360242307,\n"," 'confid_interv': 10.595998746453736,\n"," 'anti_comintern': 10.564289886726398,\n"," 'raw_materi': 10.51705911519841,\n"," 'fourier_transform': 10.4584952227038,\n"," 'philosoph_stone': 10.455260479104501,\n"," 'ural_mountain': 10.426786362976463,\n"," 'anti_semit': 10.414295418793266,\n"," 'suprem_court': 10.399601533650234,\n"," 'littl_boy': 10.355401729739699,\n"," 'cathol_church': 10.32667001119638,\n"," 'black_hole': 10.316014903377258,\n"," 'concentr_camp': 10.310870569769326,\n"," 'pave_way': 10.306492129258752,\n"," 'austria_hungari': 10.30649212925875,\n"," 'southeast_asia': 10.242529070526787,\n"," 'chief_staff': 10.225192027156904,\n"," 'game_throne': 10.223436389531193,\n"," 'orthodox_church': 10.215638698807638,\n"," 'roll_stone': 10.199920465060709,\n"," 'lorentz_transform': 10.195460816870007,\n"," 'artifici_intellig': 10.175414874668249,\n"," 'nuclear_weapon': 10.172636382523962,\n"," 'motion_pictur': 10.11454082148644,\n"," 'euclidean_geometri': 10.087784754730158,\n"," 'commit_suicid': 10.036282671132254,\n"," 'uncondit_surrend': 10.03148508175888,\n"," 'prime_minist': 10.028065673007978,\n"," 'grover_algorithm': 9.99636748098527,\n"," 'starch_sourc': 9.952855174644052,\n"," 'hidden_variabl': 9.95285517464405,\n"," 'aircraft_carrier': 9.83571207805291,\n"," 'random_variabl': 9.834210678145432,\n"," 'death_hallow': 9.815351650894117,\n"," 'weimar_republ': 9.791918956428994,\n"," 'grammi_award': 9.74396701765735,\n"," 'treati_brest': 9.695057417176406,\n"," 'counter_revolutionari': 9.655387271746383,\n"," 'schrödinger_equat': 9.639194695740288,\n"," 'east_indi': 9.599374069121096,\n"," 'hilbert_space': 9.572200669390872,\n"," 'french_indochina': 9.563128763894396,\n"," 'indo_european': 9.559258199638718,\n"," 'neural_network': 9.554181216974662,\n"," 'integr_circuit': 9.547708424886855,\n"," 'artifici_neuron': 9.531558684893525,\n"," 'eastern_bloc': 9.495969928122465,\n"," 'hiroshima_nagasaki': 9.463513297470426,\n"," 'speech_recognit': 9.439073110099818,\n"," 'radio_station': 9.438156555525715,\n"," 'analyt_engin': 9.435775146203717,\n"," 'spanish_civil': 9.411189507925444,\n"," 'billion_dollar': 9.401129003396825,\n"," 'russian_sfsr': 9.376280459415614,\n"," 'academi_scienc': 9.320991698953867,\n"," 'stock_market': 9.289560063523913,\n"," 'latin_america': 9.284721506587534,\n"," 'plebiscit_held': 9.209630590006162,\n"," 'languag_spoken': 9.188797086588998,\n"," 'craft_brewer': 9.187935518906842,\n"," 'non_abelian': 9.17629340613205,\n"," 'vladimir_lenin': 9.174829484097877,\n"," 'royal_navi': 9.174041833235101,\n"," 'video_game': 9.168988605508815,\n"," 'gold_standard': 9.155558094803625,\n"," 'south_korea': 9.15336237035942,\n"," 'east_prussia': 9.142516394147625,\n"," 'golden_age': 9.13333238421714,\n"," 'ethnic_minor': 9.120219133728956,\n"," 'densiti_matrix': 9.115935795044308,\n"," 'citi_danzig': 9.115350641906744,\n"," 'craft_breweri': 9.114473356965824,\n"," 'feder_reserv': 9.057060455169951,\n"," 'box_offic': 9.032252268717079,\n"," 'money_suppli': 9.0301609008115,\n"," 'pacif_fleet': 8.993138770109399,\n"," 'curv_spacetim': 8.9849648386637,\n"," 'czech_republ': 8.979991304581567,\n"," 'gave_birth': 8.936065516680198,\n"," 'adolf_hitler': 8.931066485878764,\n"," 'financi_crisi': 8.923583382695956,\n"," 'float_point': 8.921828279023426,\n"," 'displaystyl_hat': 8.889543232844781,\n"," 'atom_bomb': 8.880807385780399,\n"," 'byzantin_empir': 8.876206856280971,\n"," 'emperor_hirohito': 8.860971405396278,\n"," 'sphere_influenc': 8.856930754645516,\n"," 'uncondit_secur': 8.834004357796008,\n"," 'ferment_sugar': 8.823643846190286,\n"," 'roman_empir': 8.821065302088511,\n"," 'artifici_neural': 8.816715901625459,\n"," 'aggress_pact': 8.807913934437204,\n"," 'energi_momentum': 8.773059929177679,\n"," 'elementari_particl': 8.761921817338553,\n"," 'counter_offens': 8.738910627308666,\n"," 'kill_wound': 8.732670186142796,\n"," 'vice_presid': 8.732256035130654,\n"," 'public_opinion': 8.718040146051727,\n"," 'gravit_wave': 8.717048211450159,\n"," 'best_sell': 8.625235967678833,\n"," 'treati_versaill': 8.614887068492424,\n"," 'middl_east': 8.606463493907416,\n"," 'labour_camp': 8.598672880752062,\n"," 'provision_govern': 8.589918323923724,\n"," 'winston_churchil': 8.576758104323622,\n"," 'th_centuri': 8.525711230201594,\n"," 'new_zealand': 8.52222082031419,\n"," 'new_jersey': 8.522220820314189,\n"," 'new_york': 8.507574044349788,\n"," 'presidenti_elect': 8.504709363553268,\n"," 'air_raid': 8.486287374725125,\n"," 'speed_light': 8.46950410830051,\n"," 'differenti_equat': 8.465584647490122,\n"," 'pari_peac': 8.462396660386128,\n"," 'data_mine': 8.457125792092304,\n"," 'ottoman_empir': 8.444095843643185,\n"," 'electromagnet_field': 8.432023011342611,\n"," 'linear_map': 8.428747879309748,\n"," 'great_depress': 8.413312484548237,\n"," 'secur_council': 8.392494647155974,\n"," 'middl_age': 8.370508622129906,\n"," 'linear_algebra': 8.36345641905803,\n"," 'north_america': 8.33680731160287,\n"," 'michael_jackson': 8.305313938061472,\n"," 'fifth_season': 8.294519487592677,\n"," 'refer_frame': 8.280785186379584,\n"," 'theodor_roosevelt': 8.272432964845075,\n"," 'unemploy_rate': 8.27086821952803,\n"," 'attack_pearl': 8.264236920656634,\n"," 'central_committe': 8.231215814784674,\n"," 'albert_einstein': 8.221168198764746,\n"," 'vector_space': 8.204595000108865,\n"," 'dutch_east': 8.187604283676164,\n"," 'equival_billion': 8.171541080656175,\n"," 'wide_varieti': 8.162445759642045,\n"," 'th_anniversari': 8.152686793179717,\n"," 'year_ago': 8.144892865969375,\n"," 'time_dilat': 8.131233683513399,\n"," 'ture_machin': 8.12862643402205,\n"," 'natur_resourc': 8.127578344589185,\n"," 'north_caucasus': 8.126096695314057,\n"," 'ferment_yeast': 8.120625583947417,\n"," 'deep_learn': 8.116674770012642,\n"," 'key_distribut': 8.110094916455246,\n"," 'reinforc_learn': 8.10450983310591,\n"," 'supervis_learn': 8.098313567142005,\n"," 'st_centuri': 8.09033166163382,\n"," 'militari_personnel': 8.070424771025225,\n"," 'field_marshal': 8.069452931957901,\n"," 'red_armi': 8.0656168283097,\n"," 'second_sino': 8.063173869067755,\n"," 'elvi_presley': 8.056156109344442,\n"," 'quantum_cryptographi': 8.046286286036729,\n"," 'displaystyl_psi': 8.032147628287582,\n"," 'input_output': 8.024456761494902,\n"," 'non_aggress': 8.01875212914557,\n"," 'domest_cat': 8.015629383145907,\n"," 'sino_japanes': 8.00983079298182,\n"," 'soviet_union': 8.007284110871474,\n"," 'wide_rang': 7.997243168442136,\n"," 'leagu_nation': 7.996059673209217,\n"," 'visual_effect': 7.971584904358227,\n"," 'command_chief': 7.964664476933683,\n"," 'non_euclidean': 7.963299682797851,\n"," 'sign_armistic': 7.95599488217462,\n"," 'white_hous': 7.952855174644052,\n"," 'scalar_field': 7.946596184172369,\n"," 'north_africa': 7.943815574700643,\n"," 'foreign_minist': 7.939049375119021,\n"," 'appl_ii': 7.933033733731307,\n"," 'gave_rise': 7.9295226708134265,\n"," 'ingredi_beer': 7.895833070490482,\n"," 'climat_chang': 7.85236942200652,\n"," 'took_place': 7.847532056643267,\n"," 'displaystyl_lambda': 7.839502550345186,\n"," 'low_cost': 7.831381036697003,\n"," 'york_citi': 7.825844024711758,\n"," 'probabl_distribut': 7.823100611677274,\n"," 'special_relat': 7.817348855857835,\n"," 'german_shepherd': 7.816358299410981,\n"," 'music_video': 7.794380193945278,\n"," 'machin_learn': 7.776878785660333,\n"," 'descript_statist': 7.773759587310154,\n"," 'free_fall': 7.768860787897314,\n"," 'craft_beer': 7.758329546740548,\n"," 'african_american': 7.720376419288918,\n"," 'fourth_largest': 7.712872766407143,\n"," 'solv_problem': 7.704456115178655,\n"," 'store_program': 7.703739660276204,\n"," 'deep_neural': 7.693207017126813,\n"," 'displaystyl_rangl': 7.691603855232874,\n"," 'diplomat_relat': 7.684082324994371,\n"," 'nazi_parti': 7.63073408998436,\n"," 'thought_experi': 7.619431440918859,\n"," 'presid_franklin': 7.594752511380719,\n"," 'vast_major': 7.561697856131158,\n"," 'life_expect': 7.550734127332179,\n"," 'franc_belgium': 7.542458291894375,\n"," 'unit_kingdom': 7.536229401208367,\n"," 'atom_weapon': 7.531558684893525,\n"," 'euclidean_space': 7.511210039501723,\n"," 'sampl_space': 7.4831956633321255,\n"," 'communist_parti': 7.474614888067078,\n"," 'anti_aircraft': 7.473141998668202,\n"," 'peac_treati': 7.462396660386128,\n"," 'world_cup': 7.46123019042504,\n"," 'perman_member': 7.450170358052871,\n"," 'flower_plant': 7.449880514748854,\n"," 'gravit_field': 7.444962067050108,\n"," 'larg_scale': 7.4188121376160705,\n"," 'fifth_largest': 7.408371743277966,\n"," 'far_east': 7.40555079998142,\n"," 'desktop_comput': 7.396719025117113,\n"," 'versaill_treati': 7.3924946471559725,\n"," 'pure_mathemat': 7.388530029767946,\n"," 'japanes_puppet': 7.358231099230373,\n"," 'peac_confer': 7.350041167050003,\n"," 'south_africa': 7.34853332296084,\n"," 'franklin_roosevelt': 7.3239632654851565,\n"," 'new_deal': 7.314625400905822,\n"," 'south_african': 7.300792734014644,\n"," 'quantum_mechan': 7.26530314058509,\n"," 'million_copi': 7.259528650288166,\n"," 'armistic_novemb': 7.25145076570079,\n"," 'physic_quantiti': 7.2475984402051825,\n"," 'foreign_polici': 7.224157196461446,\n"," 'invad_poland': 7.218429361348374,\n"," 'gaug_theori': 7.178033745111824,\n"," 'invas_poland': 7.1697857689885645,\n"," 'entangl_particl': 7.164359279837763,\n"," 'mark_begin': 7.1503729273414685,\n"," 'present_day': 7.125511939307444,\n"," 'problem_solv': 7.119493614457497,\n"," 'squar_function': 7.100041251791325,\n"," 'small_scale': 7.072824039101475,\n"," 'strateg_bomb': 7.061789546197215,\n"," 'britain_franc': 7.055505610493173,\n"," 'east_africa': 7.044905597521202,\n"," 'anti_communist': 7.043457723424957,\n"," 'chines_communist': 7.043457723424957,\n"," 'black_white': 7.036378530206335,\n"," 'econom_crisi': 7.034800861294505,\n"," 'sign_treati': 7.0276327562632765,\n"," 'hous_common': 7.02438185715487,\n"," 'social_democrat': 7.023969952786363,\n"," 'decis_make': 7.01792263131782,\n"," 'brew_compani': 7.0138590884824,\n"," 'east_asia': 7.001737738364831,\n"," 'black_sea': 6.987050817842766,\n"," 'award_best': 6.951679543688689,\n"," 'australia_new': 6.937258319593033,\n"," 'puppet_state': 6.912006869797281,\n"," 'social_secur': 6.897729955406396,\n"," 'democrat_parti': 6.894750254985603,\n"," 'arm_forc': 6.885128551454974,\n"," 'nativ_american': 6.864285830687631,\n"," 'wave_function': 6.861181618866105,\n"," 'polynomi_time': 6.823111388151068,\n"," 'central_eastern': 6.819475905606405,\n"," 'american_kennel': 6.814703801837071,\n"," 'jewish_popul': 6.813452117978633,\n"," 'breed_standard': 6.809879013116804,\n"," 'axi_power': 6.8020875256233175,\n"," 'cultur_revolut': 6.80138293622681,\n"," 'purchas_power': 6.800177935614686,\n"," 'eastern_europ': 6.771975482002475,\n"," 'rank_th': 6.769358153628211,\n"," 'bomb_hiroshima': 6.768367878998848,\n"," 'seiz_power': 6.7662306036913495,\n"," 'linear_equat': 6.740691885624489,\n"," 'south_west': 6.738324871080573,\n"," 'mass_energi': 6.72517740555274,\n"," 'formul_quantum': 6.719304963423081,\n"," 'war_ii': 6.700523770417291,\n"," 'dog_breed': 6.693589238626169,\n"," 'execut_order': 6.6917822851435425,\n"," 'civil_war': 6.687433770976849,\n"," 'hungarian_armi': 6.675517230780759,\n"," 'unit_state': 6.675188051967467,\n"," 'western_alli': 6.6742239137592385,\n"," 'high_speed': 6.648968563901528,\n"," 'import_role': 6.6481363697889115,\n"," 'nation_park': 6.645562426125084,\n"," 'air_forc': 6.628014117164998,\n"," 'best_known': 6.618976977207151,\n"," 'central_power': 6.613556645300061,\n"," 'classic_algorithm': 6.609344357876026,\n"," 'general_relat': 6.602733785113109,\n"," 'launch_invas': 6.59194131161458,\n"," 'territori_gain': 6.575056521526349,\n"," 'sign_anti': 6.573972440123738,\n"," 'non_linear': 6.561583562016841,\n"," 'high_level': 6.554294414193002,\n"," 'televis_seri': 6.542458291894375,\n"," 'militari_equip': 6.534371870785018,\n"," 'occupi_territori': 6.529315976460378,\n"," 'late_th': 6.487274460038542,\n"," 'kingdom_franc': 6.474495701397517,\n"," 'latin_american': 6.457810344912213,\n"," 'imperi_japanes': 6.457289769953039,\n"," 'russian_civil': 6.422084149028738,\n"," 'cold_war': 6.419237660027278,\n"," 'high_school': 6.410162313121717,\n"," 'year_old': 6.381293649047663,\n"," 'outbreak_world': 6.3787680302330685,\n"," 'live_action': 6.369671002516631,\n"," 'anglo_german': 6.366325378775935,\n"," 'declar_independ': 6.360837916388945,\n"," 'perform_task': 6.356179253359105,\n"," 'music_award': 6.347332299763812,\n"," 'general_staff': 6.334077960930651,\n"," 'learn_task': 6.332778820779026,\n"," 'quantum_graviti': 6.29121131131879,\n"," 'anim_right': 6.290076772691972,\n"," 'natur_languag': 6.281906490980477,\n"," 'play_import': 6.266963765072115,\n"," 'beer_style': 6.243756373910788,\n"," 'probabl_theori': 6.228074427611418,\n"," 'nazi_germani': 6.2261187127947295,\n"," 'accord_historian': 6.217756883068468,\n"," 'east_west': 6.209630590006164,\n"," 'learn_algorithm': 6.2085207178168815,\n"," 'quantum_entangl': 6.207715649032146,\n"," 'person_comput': 6.149296366593379,\n"," 'york_time': 6.145733253208512,\n"," 'british_empir': 6.1041374611588495,\n"," 'train_data': 6.096914985581517,\n"," 'southern_russia': 6.088830564881711,\n"," 'field_equat': 6.076542356744222,\n"," 'roosevelt_won': 6.053382050598016,\n"," 'real_number': 6.04080814501101,\n"," 'treati_sign': 6.0276327562632765,\n"," 'general_elect': 6.011463031923725,\n"," 'second_largest': 6.005523653007035,\n"," 'war_crime': 6.004200160748432,\n"," 'quantum_algorithm': 5.9947363954063455,\n"," 'conserv_parti': 5.9896285112256695,\n"," 'program_languag': 5.983908412891854,\n"," 'sold_million': 5.97089665878889,\n"," 'elect_presid': 5.962182129152508,\n"," 'mid_th': 5.956289580376213,\n"," 'general_secretari': 5.955566337676922,\n"," 'estim_million': 5.949076122099434,\n"," 'brew_beer': 5.935003667771159,\n"," 'classic_mechan': 5.922979824946655,\n"," 'death_rate': 5.91723126491333,\n"," 'human_right': 5.91347026552806,\n"," 'republ_china': 5.906561522370115,\n"," 'europ_asia': 5.898233162610971,\n"," 'popular_cultur': 5.896616335095686,\n"," 'nuclear_power': 5.8887146102163435,\n"," 'film_industri': 5.875136607685235,\n"," 'forc_labour': 5.85913334292203,\n"," 'million_peopl': 5.845159220037647,\n"," 'declar_war': 5.8405341497778025,\n"," 'alli_victori': 5.840396780340443,\n"," 'armi_air': 5.836980653474875,\n"," 'point_view': 5.834365437773087,\n"," 'beer_brew': 5.797500144021225,\n"," 'western_europ': 5.77958866611235,\n"," 'russian_empir': 5.772458931443143,\n"," 'half_th': 5.7654672380406335,\n"," 'southern_europ': 5.76251515275341,\n"," 'franc_britain': 5.760049726967003,\n"," 'month_later': 5.7408046975290645,\n"," 'mathemat_scienc': 5.729566947603013,\n"," 'baltic_state': 5.702759824517754,\n"," 'north_east': 5.702288986407481,\n"," 'theori_gravit': 5.691046403953964,\n"," 'earli_th': 5.678755604847304,\n"," 'armi_command': 5.675517230780759,\n"," 'long_term': 5.672770316617738,\n"," 'year_earlier': 5.652349414136477,\n"," 'world_war': 5.631250935583726,\n"," 'field_theori': 5.621640396587438,\n"," 'mathemat_object': 5.598322414324759,\n"," 'post_war': 5.579702332220524,\n"," 'germani_itali': 5.5683509041371835,\n"," 'complex_number': 5.563100265933112,\n"," 'power_suppli': 5.561814557366455,\n"," 'star_war': 5.561011621155441,\n"," 'quantum_key': 5.549379961980769,\n"," 'featur_film': 5.527582705122377,\n"," 'british_french': 5.525654058475734,\n"," 'japanes_navi': 5.521236686732895,\n"," 'caus_death': 5.513232037086935,\n"," 'day_later': 5.509479151422607,\n"," 'war_effort': 5.506700501277617,\n"," 'european_union': 5.4888688717473215,\n"," 'algebra_number': 5.486313166843564,\n"," 'classic_physic': 5.482063693842207,\n"," 'classic_music': 5.476304424553522,\n"," 'releas_novemb': 5.474727462296849,\n"," 'special_effect': 5.46217343664425,\n"," 'popular_music': 5.457567861971935,\n"," 'econom_polici': 5.457281343758911,\n"," 'central_process': 5.456043228264226,\n"," 'train_exampl': 5.4235774114956,\n"," 'industri_product': 5.402760885854864,\n"," 'work_close': 5.400986602642039,\n"," 'prison_war': 5.394715340893207,\n"," 'week_later': 5.39400193400267,\n"," 'outbreak_war': 5.378595675529931,\n"," 'mark_end': 5.376701131540155,\n"," 'world_largest': 5.356200273327794,\n"," 'quantum_comput': 5.353027742707212,\n"," 'low_countri': 5.350926167489288,\n"," 'data_set': 5.325823607510685,\n"," 'larg_number': 5.307780825227326,\n"," 'germani_annex': 5.303080694609616,\n"," 'militari_leader': 5.272468546969872,\n"," 'ethnic_german': 5.260972378629706,\n"," 'european_countri': 5.248097921096019,\n"," 'armi_group': 5.247423578676468,\n"," 'oper_system': 5.24713947388749,\n"," 'secretari_state': 5.221027480905256,\n"," 'alli_invas': 5.212365557727402,\n"," 'post_soviet': 5.20522348848338,\n"," 'franc_itali': 5.197629794896933,\n"," 'central_europ': 5.197038699466585,\n"," 'learn_method': 5.185937432449755,\n"," 'film_studio': 5.185476728297385,\n"," 'forc_invad': 5.168237397451808,\n"," 'japan_surrend': 5.161494068961893,\n"," 'presid_roosevelt': 5.132343192943527,\n"," 'learn_model': 5.089507669766665,\n"," 'quantum_field': 5.08543286222098,\n"," 'ground_state': 5.073192021675432,\n"," 'world_highest': 5.056839935345707,\n"," 'number_theori': 5.033137538161036,\n"," 'germani_invad': 5.020649594494753,\n"," 'th_th': 5.010377095938509,\n"," 'theori_relat': 4.944089038130599,\n"," 'german_troop': 4.943468375037792,\n"," 'mathemat_physic': 4.933707664383238,\n"," 'quantum_system': 4.933523453313828,\n"," 'italian_armi': 4.927055997776723,\n"," 'high_success': 4.92515106856731,\n"," 'invad_soviet': 4.901350666122408,\n"," 'parti_member': 4.893300164361065,\n"," 'axi_forc': 4.887515144291854,\n"," 'film_compani': 4.8796682987733,\n"," 'feder_govern': 4.856563983309897,\n"," 'displaystyl_displaystyl': 4.821766186048375,\n"," 'public_work': 4.812842854676202,\n"," 'union_sign': 4.803143394091247,\n"," 'nation_secur': 4.786606307580268,\n"," 'enter_world': 4.781832887845837,\n"," 'japanes_attack': 4.7806441576316985,\n"," 'american_music': 4.763106996320264,\n"," 'area_mathemat': 4.751593253933013,\n"," 'hitler_order': 4.745199940567428,\n"," 'classic_comput': 4.742215591142877,\n"," 'film_produc': 4.721529628537596,\n"," 'empir_japan': 4.67606724179165,\n"," 'quantum_theori': 4.650124935720584,\n"," 'kill_million': 4.64599699737024,\n"," 'time_share': 4.645806856343157,\n"," 'germani_austria': 4.641156212073575,\n"," 'german_invas': 4.629359784609729,\n"," 'network_learn': 4.628021807523158,\n"," 'april_hitler': 4.623637960686519,\n"," 'follow_year': 4.612192287664495,\n"," 'german_armi': 4.609819598214084,\n"," 'enter_war': 4.597235962005271,\n"," 'invas_soviet': 4.575173098233687,\n"," 'soviet_troop': 4.568659791275014,\n"," 'japanes_invas': 4.566224141506204,\n"," 'quantum_physic': 4.534880392285654,\n"," 'defeat_germani': 4.499495062849984,\n"," 'western_power': 4.478249840727322,\n"," 'unit_nation': 4.453437444457432,\n"," 'second_world': 4.445405223263357,\n"," 'oper_displaystyl': 4.433974253839885,\n"," 'head_state': 4.4329060494750365,\n"," 'follow_day': 4.423499801347596,\n"," 'year_later': 4.388068978248928,\n"," 'general_consid': 4.383505863928718,\n"," 'west_germani': 4.363622236544668,\n"," 'prepar_war': 4.352123464168738,\n"," 'state_emerg': 4.331504017266823,\n"," 'mathemat_model': 4.3310175711127386,\n"," 'british_armi': 4.29700560752703,\n"," 'russian_armi': 4.275820277636402,\n"," 'japanes_citi': 4.256094617937109,\n"," 'process_unit': 4.25532051530498,\n"," 'end_th': 4.234391434298946,\n"," 'european_power': 4.1904475297445885,\n"," 'increas_number': 4.18356460735361,\n"," 'state_america': 4.16521026627138,\n"," 'entangl_state': 4.158439308293943,\n"," 'german_italian': 4.143932957439485,\n"," 'great_power': 4.142646808942885,\n"," 'death_million': 4.140356949310487,\n"," 'militari_forc': 4.108492811858749,\n"," 'space_time': 4.101486340119344,\n"," 'popul_europ': 4.061482099197459,\n"," 'popul_countri': 4.0242445426312905,\n"," 'alli_power': 4.015906626670123,\n"," 'japanes_militari': 4.013229369443685,\n"," 'million_year': 4.0081723045426365,\n"," 'popul_million': 4.000953892645072,\n"," 'alli_forc': 3.9923998737854944,\n"," 'million_million': 3.921810007925334,\n"," 'member_unit': 3.8804209512782215,\n"," 'franc_germani': 3.8689018679042277,\n"," 'end_war': 3.8623987602205077,\n"," 'militari_oper': 3.8493210456573514,\n"," 'german_forc': 3.8194309188882976,\n"," 'british_govern': 3.77083410928401,\n"," 'japanes_american': 3.7419954008842033,\n"," 'largest_film': 3.734815319287822,\n"," 'german_empir': 3.684501338802189,\n"," 'number_field': 3.6771355091791413,\n"," 'film_product': 3.66890120255648,\n"," 'develop_countri': 3.6522141339975143,\n"," 'japanes_forc': 3.6092928633980925,\n"," 'theori_quantum': 3.6028192209422265,\n"," 'anim_film': 3.5583887519035535,\n"," 'japanes_armi': 3.553226800977331,\n"," 'british_forc': 3.5510110475596974,\n"," 'later_call': 3.494528809956634,\n"," 'number_peopl': 3.469810535931636,\n"," 'million_unit': 3.4578601178714763,\n"," 'comput_quantum': 3.432462210201617,\n"," 'later_year': 3.3880689782489277,\n"," 'year_term': 3.3733618414660675,\n"," 'war_economi': 3.337296046491101,\n"," 'countri_europ': 3.323804824490196,\n"," 'new_govern': 3.29468509570178,\n"," 'exampl_includ': 3.275206553053632,\n"," 'russia_world': 3.274564515639309,\n"," 'start_war': 3.2341262547702883,\n"," 'war_began': 3.22659258208488,\n"," 'japanes_govern': 3.2197003806767093,\n"," 'major_film': 3.218643592232585,\n"," 'general_theori': 3.205619576783725,\n"," 'germani_japan': 3.1779821917504627,\n"," 'quantum_state': 3.168592024654835,\n"," 'member_state': 3.115186110340062,\n"," 'union_unit': 3.0914445585354713,\n"," 'war_japan': 3.049173044095646,\n"," 'state_displaystyl': 3.045086683995077,\n"," 'war_juli': 3.040726036773549,\n"," 'german_govern': 2.9867865607184783,\n"," 'european_state': 2.979855224259463,\n"," 'nation_govern': 2.9611690265392063,\n"," 'war_end': 2.9008729080351436,\n"," 'end_world': 2.8375423204321244,\n"," 'japanes_war': 2.836821841416466,\n"," 'german_militari': 2.8083299256550482,\n"," 'countri_world': 2.7854806648609802,\n"," 'state_popul': 2.756001845794735,\n"," 'continu_war': 2.740092720249411,\n"," 'war_germani': 2.6711743569777404,\n"," 'war_franc': 2.614561838165553,\n"," 'soviet_forc': 2.5896726679288022,\n"," 'year_year': 2.5782449557411553,\n"," 'japan_germani': 2.3154857155003974,\n"," 'nation_world': 2.26098065212593,\n"," 'war_soviet': 2.22016631292545,\n"," 'war_unit': 2.197411556672936,\n"," 'war_europ': 2.18511594610872,\n"," 'german_soviet': 2.1375066882800517,\n"," 'war_alli': 1.9244729682776978,\n"," 'state_unit': 1.7768089562144667,\n"," 'state_govern': 1.743972156933534,\n"," 'nation_state': 1.7162169796888698,\n"," 'state_includ': 1.6488929525390184,\n"," 'war_german': 1.4669980772438862,\n"," 'soviet_state': 1.3421512614663342,\n"," 'state_soviet': 1.3421512614663342,\n"," 'war_world': 1.3248636915921388,\n"," 'war_includ': 1.1085953726864304,\n"," 'war_year': 1.0347930810198065,\n"," 'state_war': 0.4321767157347729}"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["bigram_measures = nltk.collocations.BigramAssocMeasures()\n","finder = nltk.collocations.BigramCollocationFinder.from_words([word for text in train_df['text'] for word in text.split()])\n","finder.apply_freq_filter(10)\n","bigram_score = finder.score_ngrams(bigram_measures.pmi)\n","\n","\n","bigrams = {}\n","for bigram, score in bigram_score:\n","    bigram = '_'.join(bigram)\n","    bigrams[bigram] = score\n","\n","bigrams"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"ZCgRKlxgwb07"},"outputs":[{"data":{"text/plain":["{'attack_pearl_harbor': 19.570729049915386,\n"," 'new_york_citi': 16.348064845025952,\n"," 'late_th_centuri': 15.386564929432556,\n"," 'earli_th_centuri': 14.888386194853464,\n"," 'world_war_ii': 14.789785183455376,\n"," 'new_york_time': 14.667954073522704,\n"," 'second_world_war': 11.671997805348237,\n"," 'war_unit_state': 9.559794886823731}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["trigram_measures = nltk.collocations.TrigramAssocMeasures()\n","finder = nltk.collocations.TrigramCollocationFinder.from_words([word for text in train_df['text'] for word in text.split()])\n","finder.apply_freq_filter(25)\n","trigram_score = finder.score_ngrams(trigram_measures.pmi)\n","\n","# print(\"\\nTrigram score:\\n\")\n","# trigram_score[:10]\n","\n","trigrams = {}\n","for trigram, score in trigram_score:\n","    trigram = '_'.join(trigram)\n","    trigrams[trigram] = score\n","\n","trigrams"]},{"cell_type":"markdown","metadata":{},"source":["## Entrainement"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1301396,"status":"ok","timestamp":1714751877365,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"uSfhZ3lb_uMq","outputId":"00414303-19de-4a67-cf40-718f0e53bfb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training LDA model on data...\n","Coherence Score on Training Data: 0.5345474529003926\n"]}],"source":["# modèle LDA\n","def train_lda(data, num_topics=10, passes=200) -> tuple:\n","    \"\"\"\n","    Train LDA model on the data\n","\n","    Parameters:\n","    data : DataFrame : data to train the model on\n","    num_topics : int : number of topics to train the model on\n","    passes : int : number of passes for training the model\n","\n","    Returns:\n","    tuple : (lda_model, dictionary, bow_corpus)\n","        lda_model : LdaModel : trained LDA model\n","        dictionary : Dictionary : Gensim dictionary\n","        bow_corpus : list : bag of words corpus\n","    \"\"\"\n","    processed_docs = [preprocess_string(doc) for doc in data['text']]\n","    dictionary = Dictionary(processed_docs)\n","    dictionary.filter_extremes(no_below=20, no_above=0.85, keep_n=1000)\n","    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","    lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=passes, random_state=42)\n","    return lda_model, dictionary, bow_corpus, processed_docs\n","\n","\n","def evaluate_model(model, texts: list, dictionary) -> float:\n","    \"\"\"\n","    Evaluate the LDA model using Coherence Score\n","\n","    Parameters:\n","    model : LdaModel : trained LDA model\n","    texts : list : list of texts\n","    dictionary : Dictionary : Gensim dictionary\n","\n","    Returns:\n","    float : coherence score of the model\n","    \"\"\"\n","    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n","    return coherence_model.get_coherence()\n","\n","\n","data_df = train_df\n","print(\"Training LDA model on data...\")\n","lda_model, dictionary, bow_corpus, processed_docs = train_lda(data_df, 30, 250)\n","print(\"Coherence Score on Training Data:\", evaluate_model(lda_model, processed_docs, dictionary))\n"]},{"cell_type":"markdown","metadata":{"id":"8YUu9_KPBJpK"},"source":["Ce bloc de code illustre la préparation, l'entraînement et l'évaluation d'un modèle de Latent Dirichlet Allocation (LDA) avec la bibliothèque Gensim, destiné à la détection de thèmes dans des documents textuels. Voici une explication simplifiée de chaque partie du code :\n","\n","1. **Préparation et entraînement du modèle LDA** :\n","   - **Fonction `train_lda`** : Cette fonction configure et entraîne un modèle LDA. Elle prend en entrée les documents textuels, le nombre de sujets désirés (`num_topics`), et le nombre de passages sur le corpus (`passes`) pour l'optimisation.\n","   - **Dictionnaire et corpus** : Les documents sont d'abord transformés en une liste de tokens (`processed_docs`), puis en un dictionnaire (`dictionary`) qui assigne un identifiant unique à chaque mot. Le corpus est converti en format sac-de-mots (`bow_corpus`), où chaque document est représenté par une liste de paires (identifiant de mot, fréquence).\n","   - **Paramètres du modèle** : Le modèle est paramétré pour extraire un certain nombre de sujets (`num_topics`), avec une association explicite entre les mots et leurs identifiants (`id2word=dictionary`), et est optimisé à travers plusieurs passes sur les données.\n","\n","2. **Évaluation de la cohérence du modèle** :\n","   - **Fonction `evaluate_model`** : Cette fonction utilise `CoherenceModel` pour évaluer la cohérence du modèle LDA, un indicateur de la qualité des thèmes extraits. Une cohérence élevée signifie que les mots fréquents d'un sujet apparaissent souvent ensemble dans les documents, suggérant que le sujet est significatif et cohérent.\n","   - **Arguments** : Le modèle LDA (`model`), les documents prétraités (`texts`), et le dictionnaire sont nécessaires pour calculer la cohérence, qui se base sur la fréquence de co-occurrence des mots.\n","\n","3. **Exécution et résultats** :\n","   - **Entraînement et évaluation** : Le modèle est entraîné et évalué sur le `train_df` après l'extraction et la préparation des documents. La cohérence des thèmes générés est ensuite affichée, fournissant un feedback sur la pertinence des thèmes identifiés.\n","\n","4. **Implications pratiques** :\n","   - **Temps de calcul** : L'entraînement d'un modèle LDA, en particulier avec un grand nombre de passes et de sujets, peut être long (indiqué par \"47 minutes\"), et dépend fortement de la taille du corpus et des ressources système disponibles.\n","   - **Ajustement des paramètres** : Le score de cohérence aide à ajuster les paramètres du modèle pour améliorer la pertinence des sujets extraits, en fonction des objectifs de l'analyse.\n","\n","Ce processus est crucial pour l'analyse de grands ensembles de textes où la détection de thèmes sous-jacents est nécessaire pour la compréhension ou la classification des documents."]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43138,"status":"ok","timestamp":1714752281859,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"oOCVqENYUIDf","outputId":"cd4ba55e-733d-4447-9d99-f4320f4d960c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'frame': 0.0145837795, 'event': 0.0054012015, 'clear': 0.0017843783, 'local': 0.0016530053, 'addit': 0.0007509848}, {'record': 0.027270636, 'relea': 0.019521128, 'album': 0.01541515, 'beatl': 0.013352306, 'singl': 0.011891185, 'hollywood': 0.011616089, 'studio': 0.009913859, 'includ': 0.009185504, 'tour': 0.005398599, 'origin': 0.004929646}, {'churchil': 0.028603062, 'parti': 0.017305389, 'conserv': 0.007528526, 'gener': 0.0074543105, 'labour': 0.0035542364}, {'nation': 0.037965335, 'product': 0.022768166, 'increa': 0.020536266, 'govern': 0.015615228}, {'british': 0.030386414, 'forc': 0.018510345, 'occupi': 0.009239134, 'empir': 0.00803592, 'march': 0.0072682453, 'attack': 0.006636343, 'line': 0.0064065317, 'april': 0.005773251, 'earli': 0.0047080503, 'follow': 0.003633895}]\n"]}],"source":["# Extraire les mots-clé pour chaque document\n","def extract_keywords_for_document(lda_model, bow_corpus: list, documents: list, num_keywords: int) -> list:\n","    \"\"\"\n","    Extract keywords for each document\n","\n","    Parameters:\n","    lda_model : LdaModel : trained LDA model\n","    bow_corpus : list : bag of words corpus\n","    documents : list : list of documents\n","    num_keywords : int : number of keywords to extract\n","\n","    Returns:\n","    list : list of dictionaries containing keywords and their scores for each document\n","    \"\"\"\n","    document_keywords_scores = []\n","    for bow_doc, doc_text in zip(bow_corpus, documents):\n","        doc_topics = lda_model.get_document_topics(bow_doc, minimum_probability=0.01)\n","        doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n","        keywords_scores = {}\n","        for topic_num, topic_weight in doc_topics:\n","            topic_words = lda_model.show_topic(topic_num, topn=100)\n","            for word, word_weight in topic_words:\n","                if word in doc_text:\n","                    if word not in keywords_scores:\n","                        keywords_scores[word] = word_weight * topic_weight\n","                    else:\n","                        keywords_scores[word] += word_weight * topic_weight\n","\n","        keywords_scores_sorted = dict(sorted(keywords_scores.items(), key=lambda item: item[1], reverse=True))\n","        top_keywords_scores = dict(list(keywords_scores_sorted.items())[:num_keywords])\n","        document_keywords_scores.append(top_keywords_scores)\n","\n","    return document_keywords_scores\n","\n","\n","# Extraire les mots-clés et leurs scores avec la fonction modifiée\n","keywords_and_scores = extract_keywords_for_document(lda_model, bow_corpus, processed_docs, num_keywords=10)\n","print(keywords_and_scores[:5])\n"]},{"cell_type":"markdown","metadata":{"id":"-CB1WDVTUI2S"},"source":["Ce bloc de code permet d'extraire les mots-clés les plus pertinents pour chaque document d'un corpus, en utilisant un modèle de Latent Dirichlet Allocation (LDA) déjà entraîné. Voici une explication simple et structurée de chaque étape de ce processus :\n","\n","1. **Définition de la fonction d'extraction** :\n","   - **`extract_keywords_for_document`** : Cette fonction est configurée pour prendre en entrée un modèle LDA, un corpus en format sac de mots (`bow_corpus`), les documents sous forme textuelle, et le nombre désiré de mots-clés à extraire (`num_keywords`).\n","\n","2. **Extraction de mots-clés par document** :\n","   - **Récupération des distributions des sujets** : Pour chaque document, le modèle LDA est consulté pour obtenir une distribution des sujets (avec un seuil de probabilité minimale) qui indique la contribution relative de chaque sujet au document.\n","   - **Tri des sujets** : Les sujets sont triés par leur poids (importance) pour prioriser les sujets les plus influents pour le document en question.\n","\n","3. **Sélection et pondération des mots-clés** :\n","   - **Extraction des termes par sujet** : Pour chaque sujet pertinent, les termes les plus significatifs sont extraits à l'aide de la méthode `show_topic`, qui renvoie les mots et leur poids dans ce sujet particulier.\n","   - **Calcul du score des mots-clés** : Le score de chaque mot-clé est ajusté en fonction de son poids dans le sujet et le poids de ce sujet dans le document. Ce calcul assure que les mots-clés sélectionnés sont non seulement centraux pour les sujets mais aussi spécifiquement pertinents pour le document.\n","\n","4. **Filtrage et classement des mots-clés** :\n","   - **Match avec le texte du document** : Seuls les mots qui figurent effectivement dans le document sont conservés. Cela garantit la pertinence contextuelle des mots-clés.\n","   - **Classement des mots-clés** : Les mots-clés sont ensuite triés par leur score total, et les `num_keywords` les plus pertinents sont retenus pour chaque document.\n","\n","5. **Résultats et vérifications** :\n","   - **Compilation des résultats** : La liste des mots-clés pour chaque document est stockée dans une liste de dictionnaires, facilitant l'analyse ou l'affichage ultérieur.\n","   - **Affichage des premiers résultats** : Pour vérification, les mots-clés des premiers documents sont imprimés, montrant ainsi les termes les plus influents et leur applicabilité aux textes analysés.\n","\n","Cette méthode offre une manière systématique et quantifiable de déterminer les termes clés qui caractérisent le contenu de documents divers, en se basant sur une analyse statistique des thèmes via LDA. Elle est essentielle pour des tâches comme la résumé automatique, la catégorisation de contenu ou encore l'analyse de tendances dans de grands ensembles de données textuelles."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17923,"status":"ok","timestamp":1714752299774,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"KLKnmIsownl-","outputId":"69fe09f5-6f78-4f14-e6d6-fabd5db12b66"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'frame': 0.0145837795, 'event': 0.0054012015, 'clear': 0.0017843783, 'local': 0.0016530053, 'addit': 0.0007509848}, {'record': 0.027270636, 'relea': 0.019521128, 'album': 0.01541515, 'beatl': 0.013352306, 'singl': 0.011891185, 'hollywood': 0.011616089, 'studio': 0.009913859, 'includ': 0.009185504, 'tour': 0.005398599, 'origin': 0.004929646}, {'churchil': 0.028603062, 'parti': 0.017305389, 'conserv': 0.007528526, 'gener': 0.0074543105, 'labour': 0.0035542364, 'conserv_parti': 0.001}, {'nation': 0.037965335, 'product': 0.022768166, 'increa': 0.020536266, 'govern': 0.015615228, 'nation_govern': 0.001}, {'british': 0.030386414, 'forc': 0.018510345, 'occupi': 0.009239134, 'empir': 0.00803592, 'march': 0.0072682453, 'attack': 0.006636343, 'line': 0.0064065317, 'april': 0.005773251, 'earli': 0.0047080503, 'follow': 0.003633895, 'british_empir': 0.001, 'british_forc': 0.001}, {'hungarian': 0.0069324034, 'territori': 0.00663382, 'romania': 0.0050703166}, {'roosevelt': 0.023476351, 'presid': 0.015884107, 'pacif': 0.0041887495, 'deci': 0.003725111, 'harbor': 0.0036246346, 'pearl': 0.0031173378, 'fleet': 0.0028520024, 'admir': 0.0024621226, 'short': 0.0016125379, 'command': 0.0011859024, 'presid_roosevelt': 0.001, 'pearl_harbor': 0.001, 'pacif_fleet': 0.001}, {'dai': 0.009975085, 'franc': 0.008873711, 'britain': 0.008540802, 'later': 0.0040249745, 'remain': 0.0039614206, 'attack': 0.0037713833, 'home': 0.003482039, 'princ': 0.0025538532, 'crisi': 0.0018473642, 'london': 0.0015605286, 'britain_franc': 0.001, 'franc_britain': 0.001}, {'death': 0.0138084935, 'preslei': 0.013751119, 'plai': 0.009550435, 'new': 0.009211326, 'offer': 0.005563736, 'perman': 0.004466846, 'month': 0.0038197322, 'result': 0.003463017, 'georg': 0.0032258427, 'charg': 0.003111902}, {'german': 0.036078997, 'armi': 0.021830318, 'british': 0.01847369, 'alli': 0.010658486, 'french': 0.008968241, 'septemb': 0.0064198268, 'command': 0.006246413, 'line': 0.0055226614, 'launch': 0.0050799483, 'posit': 0.003458285, 'british_french': 0.001, 'armi_command': 0.001, 'german_armi': 0.001, 'british_armi': 0.001}]\n"]}],"source":["# Verify if the combination of keywords and permutation is present in bigrams and trigrams dictionary. If present, add the bigram/trigam to the keyword\n","def verification_bigram_trigram(keyword: dict, bigrams: dict, trigrams: dict) -> dict:\n","    \"\"\"\n","    Verify if the combination of keywords and permutation is present in bigrams and trigrams dictionary. If present, add the bigram/trigam to the keyword\n","\n","    Parameters:\n","    keyword : dict : dictionary containing keywords and their scores\n","    bigrams : dict : dictionary containing bigrams and their scores\n","    trigrams : dict : dictionary containing trigrams and their scores\n","\n","    Returns:\n","    dict : updated dictionary containing keywords and their scores\n","    \"\"\"\n","    keywords = list(keyword.keys())\n","    bigrams = set(bigrams.keys())\n","    trigrams = set(trigrams.keys())\n","\n","    combined_keywords = set()\n","    for k in range(1, min(4, len(keywords) + 1)):  # Use min to ensure we do not exceed 3 tokens\n","        for subset in combinations(keywords, k):\n","            for perm in permutations(subset):\n","                combined_keyword = '_'.join(perm)\n","                if len(combined_keyword.split()) <= 3:  # Check the token count\n","                    combined_keywords.add(combined_keyword)\n","\n","    for combined_keyword in combined_keywords:\n","        if combined_keyword in bigrams or combined_keyword in trigrams:\n","            keyword[combined_keyword] = 0.001  # Add a small value to indicate it is a bigram/trigram\n","\n","    return keyword\n","\n","# Vérifier si la combinaison de mots-clés et la permutation sont présentes dans le dictionnaire des bigrams et trigrams\n","for i in range(len(keywords_and_scores)):\n","    keywords_and_scores[i] = verification_bigram_trigram(keywords_and_scores[i], bigrams, trigrams)\n","\n","print(keywords_and_scores[:10])\n"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6054,"status":"ok","timestamp":1714752305820,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"YuNF2ixWo6Ge","outputId":"7f5675c9-e8d4-4a2b-8f4e-2e21062a3462"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                           sentences  \\\n","0  It is clear that the two events that are simul...   \n","1  When the Beatles' studio albums were released ...   \n","2  In May, Churchill was still generally unpopula...   \n","3  Government spending increased from 8.0% of the...   \n","4  In early 1918, the front line was extended and...   \n","\n","                                                tags  \\\n","0                                            [event]   \n","1  [past mast, magical mystery tour, singl, apple...   \n","2                                     [labour parti]   \n","3                                       [under hoov]   \n","4  [extend, jordan valley, first transjordan, sec...   \n","\n","                                            keywords  match match percent  \\\n","0  {'frame': 0.0145837795, 'event': 0.0054012015,...      1        100.0%   \n","1  {'record': 0.027270636, 'relea': 0.019521128, ...      1         20.0%   \n","2  {'churchil': 0.028603062, 'parti': 0.017305389...      0          0.0%   \n","3  {'nation': 0.037965335, 'product': 0.022768166...      0          0.0%   \n","4  {'british': 0.030386414, 'forc': 0.018510345, ...      0          0.0%   \n","\n","   precision  recall   f_score  \n","0        0.2     1.0  0.333333  \n","1        0.1     0.2  0.133333  \n","2        0.0     0.0  0.000000  \n","3        0.0     0.0  0.000000  \n","4        0.0     0.0  0.000000  \n"]}],"source":["# Récupérer les colonnes\n","def get_text_columns(dataset: str, dataframe) -> list:\n","  \"\"\"\n","    Get the text columns and tags from the dataframe\n","\n","    Parameters:\n","    dataset : str : dataset name\n","    dataframe : DataFrame : dataframe to get the columns from\n","\n","    Returns:\n","    if dataset == \"Stackoverflow\":\n","        list: list of titles, bodys and tags\n","    elif dataset == \"Wikipedia\":\n","        list: list of sentences and keywords\n","  \"\"\"\n","  if dataset == \"Stackoverflow\":\n","    titles = dataframe['title'].tolist()\n","    bodys = dataframe['body'].tolist()\n","    tags = [ ' '.join([lemmatize_stemming(word) for word in word_tokenize(tag.replace('|', ' ').lower())]) for tag in dataframe['tags'].tolist()]\n","    return titles, bodys, tags\n","  elif dataset == \"Wikipedia\":\n","    sentences = dataframe['sentence'].tolist()\n","    tags = [[lemmatize_stemming(keyword.lower()) for keyword in keyword_list] for keyword_list in dataframe['keywords']]\n","    return sentences, tags\n","\n","def calculate_precision_recall_fscore(true_tags: list, predicted_keywords: dict) -> tuple:\n","    \"\"\"\n","    Calculate precision, recall and f-score for the predicted keywords\n","\n","    Parameters:\n","    true_tags : list : list of true tags\n","    predicted_keywords : dict : dictionary of predicted keywords\n","\n","    Returns:\n","    tuple : (precision, recall, f_score)\n","        precision : float : precision score\n","        recall : float : recall score\n","        f_score : float : f-score\n","    \"\"\"\n","    true_set = set(true_tags)\n","    predicted_set = set(predicted_keywords.keys())  # Les clés sont les mots-clés\n","    tp = len(true_set & predicted_set)  # True positives\n","    fp = len(predicted_set - true_set)  # False positives\n","    fn = len(true_set - predicted_set)  # False negatives\n","\n","    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    if precision + recall > 0:\n","        f_score = 2 * precision * recall / (precision + recall)\n","    else:\n","        f_score = 0\n","\n","    return precision, recall, f_score\n","\n","def matches_keywords(keyword_scores: list, tags: list) -> list:\n","    \"\"\"\n","    Check if the keywords match the tags.\n","\n","    Parameters:\n","    keyword_scores : list : list of dictionaries containing keywords and their scores\n","    tags : list : list of tags\n","\n","    Returns:\n","    list : list of numbers indicating the count of keyword matches per tag list\n","    \"\"\"\n","    pourcentages:list=[]\n","    matches = []\n","    for tag_list, keyword_dict in zip(tags, keyword_scores):\n","        keywords = list(keyword_dict.keys())\n","        # print('keywords: ', keywords, '\\ntag_list: ', tag_list)\n","        tag_ngram = []\n","        # S'il y a des tags de bigrams ou trigrams séparés par espace, on le remplace par '_'\n","        for tag in tag_list:\n","            tag = tag.strip().replace(' ', '_')\n","            # print(tag)\n","            tag_ngram.append(tag)\n","\n","        match = 0\n","        bingo_pourcentage:str = \"\"\n","        for keyword in keywords:\n","            if keyword in tag_ngram:\n","                match += 1\n","        bingo_pourcentage = f\"{round(match / len(tag_ngram), 3) * 100}%\"\n","        pourcentages.append(bingo_pourcentage)\n","        matches.append(match)\n","    return matches, pourcentages\n","\n","\n","if dataset == \"Stackoverflow\":\n","  titles, bodys, tags = get_text_columns(dataset, data_df)\n","  tags = [tag.split(' ') for tag in tags]\n","  evaluation_scores = [calculate_precision_recall_fscore(tags[i], keywords_and_scores[i]) for i in range(len(tags))]\n","  match_keywords, match_percent = matches_keywords(keywords_and_scores, tags)\n","  dataframe_lda = pd.DataFrame(zip(titles, bodys, tags, keywords_and_scores, match_keywords, match_percent), columns=['title', 'body', 'tags', 'keywords', 'match', 'match percent'])\n","  # Add scores to DataFrame\n","  dataframe_lda['precision'], dataframe_lda['recall'], dataframe_lda['f_score'] = zip(*evaluation_scores)\n","  dataframe_lda.to_csv(\"../results/csv/Stackoverflow_lda-keywords-match-train_df.csv\", index=False)\n","\n","  # Affichage des résultats\n","  print(dataframe_lda[['title', 'tags', 'keywords', 'match', 'match percent', 'precision', 'recall', 'f_score']].head())\n","\n","elif dataset == \"Wikipedia\":\n","  sentences, tags = get_text_columns(dataset, data_df)\n","  evaluation_scores = [calculate_precision_recall_fscore(tags[i], keywords_and_scores[i]) for i in range(len(tags))]\n","  match_keywords, match_percent = matches_keywords(keywords_and_scores, tags)\n","  dataframe_lda = pd.DataFrame(zip(sentences, tags, keywords_and_scores, match_keywords, match_percent), columns=['sentences', 'tags', 'keywords', 'match', 'match percent'])\n","\n","  # Add scores to DataFrame\n","  dataframe_lda['precision'], dataframe_lda['recall'], dataframe_lda['f_score'] = zip(*evaluation_scores)\n","  dataframe_lda.to_csv(\"../results/csv/Wikipedia_lda-keywords-match-train_df.csv\", index=False)\n","\n","  # Affichage des résultats\n","  print(dataframe_lda[['sentences', 'tags', 'keywords', 'match', 'match percent','precision', 'recall', 'f_score']].head())"]},{"cell_type":"markdown","metadata":{"id":"TjYeLAzBWBak"},"source":["Ce bloc de code réalise l'extraction et le traitement des colonnes textuelles à partir de DataFrames spécifiques, évalue l'exactitude des mots-clés extraits en utilisant les métriques de précision, rappel, et F-score, puis enregistre et affiche les résultats. Voici une explication simplifiée de chaque partie :\n","\n","1. **Extraction des données textuelles** :\n","   - **`get_text_columns`** : Cette fonction adapte son comportement en fonction du dataset spécifié (`\"Stackoverflow\"` ou `\"Wikipedia\"`). Pour \"Stackoverflow\", elle récupère les titres, corps de texte, et les tags transformés pour l'analyse. Pour \"Wikipedia\", elle extrait les phrases et traite les mots-clés associés.\n","\n","2. **Calcul des métriques d'évaluation** :\n","   - **`calculate_precision_recall_fscore`** : Cette fonction calcule la précision, le rappel, et le score F (F-score) pour évaluer l'efficacité avec laquelle les mots-clés prédits correspondent aux tags réels. Elle utilise des ensembles pour déterminer les vrais positifs, faux positifs, et faux négatifs entre les tags et les mots-clés prédits.\n","\n","3. **Traitement des correspondances de mots-clés** :\n","   - **`matches_keywords`** : Cette fonction compte le nombre de correspondances entre les tags réels et les combinaison mots-clés prédits allant jusqu'à la combinaison de 3 tokens, offrant une mesure directe de la pertinence des mots-clés extraits par rapport aux tags attendus.\n","\n","4. **Création et manipulation de DataFrames** :\n","   - En fonction du dataset, les données extraites et les scores calculés sont organisés dans des DataFrames. Ces DataFrames incluent des colonnes pour les textes, les tags, les mots-clés prédits, le nombre de correspondances, et les scores de précision, rappel, et F-score.\n","\n","5. **Enregistrement et affichage des résultats** :\n","   - Les DataFrames finaux sont enregistrés en format CSV pour une utilisation future ou une analyse plus approfondie.\n","   - Les premières entrées du DataFrame sont également affichées pour donner un aperçu immédiat des résultats de l'analyse.\n","\n","Ce processus permet une évaluation systématique de l'efficacité des techniques d'extraction de mots-clés, adaptée à des contextes variés tels que les questions techniques de \"Stackoverflow\" ou les articles encyclopédiques de \"Wikipedia\". Les résultats obtenus peuvent aider à affiner les méthodes d'extraction de mots-clés ou à valider l'exactitude des modèles utilisés."]},{"cell_type":"markdown","metadata":{},"source":["## Test"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1714741044880,"user":{"displayName":"Zhongjie LI","userId":"01049570628257589342"},"user_tz":-120},"id":"FUZ867jjf2Oz"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                           sentences  \\\n","0  Laurence had interviewed Sweeney and his crew,...   \n","1  Historically, Russian athletes have been one o...   \n","2  Italian defeats prompted Germany to deploy an ...   \n","3  Genotype evolution can be modeled with the Har...   \n","4  In February 1895, Churchill was commissioned a...   \n","\n","                                                tags  \\\n","0                                 [the great artist]   \n","1             [russian athlet, olympic gam, russian]   \n","2  [deploy an expeditionary forc, rommel, afrika ...   \n","3                          [hardy-weinberg principl]   \n","4  [4th queen's own hussar, british armi, aldershot]   \n","\n","                                            keywords  match  \\\n","0  {'great': 0.012375866, 'refer': 0.0044525727, ...      0   \n","1  {'game': 0.017060347, 'success': 0.011271413, ...      1   \n","2  {'forc': 0.017408913, 'germani': 0.016419487, ...      1   \n","3   {'model': 0.0072758105, 'principl': 0.007131724}      0   \n","4  {'churchil': 0.03399625, 'armi': 0.03095527, '...      1   \n","\n","         match percent  precision    recall   f_score  \n","0                 0.0%   0.000000  0.000000  0.000000  \n","1  33.300000000000004%   0.250000  0.333333  0.285714  \n","2                11.1%   0.090909  0.111111  0.100000  \n","3                 0.0%   0.000000  0.000000  0.000000  \n","4  33.300000000000004%   0.000000  0.000000  0.000000  \n"]}],"source":["# Préparation des données de test\n","test_docs = [preprocess_string(doc) for doc in test_df['text']]\n","test_bow_corpus = [dictionary.doc2bow(doc) for doc in test_docs]\n","\n","# Extraction des mots-clés pour les documents de test\n","test_keywords_and_scores = extract_keywords_for_document(lda_model, test_bow_corpus, test_docs, num_keywords=10)\n","\n","# Vérification des bigrams et trigrams dans les mots-clés de test\n","for i in range(len(test_keywords_and_scores)):\n","    test_keywords_and_scores[i] = verification_bigram_trigram(test_keywords_and_scores[i], bigrams, trigrams)\n","\n","# Extraction des colonnes de texte et des tags pour le dataset de test\n","if dataset == \"Stackoverflow\":\n","    test_titles, test_bodys, test_tags = get_text_columns(dataset, test_df)\n","    test_tags = [tag.split(' ') for tag in test_tags]\n","    test_evaluation_scores = [calculate_precision_recall_fscore(test_tags[i], test_keywords_and_scores[i]) for i in range(len(test_tags))]\n","    test_match_keywords, test_match_percent = matches_keywords(test_keywords_and_scores, test_tags)\n","    test_dataframe_lda = pd.DataFrame(zip(test_titles, test_bodys, test_tags, test_keywords_and_scores, test_match_keywords, test_match_percent), columns=['title', 'body', 'tags', 'keywords', 'match', 'match percent'])\n","    test_dataframe_lda['precision'], test_dataframe_lda['recall'], test_dataframe_lda['f_score'] = zip(*test_evaluation_scores)\n","    test_dataframe_lda.to_csv(\"../results/csv/Stackoverflow_lda-keywords-match-test_df.csv\", index=False)\n","\n","    print(test_dataframe_lda[['title', 'tags', 'keywords', 'match', 'match percent', 'precision', 'recall', 'f_score']].head())\n","\n","elif dataset == \"Wikipedia\":\n","    test_sentences, test_tags = get_text_columns(dataset, test_df)\n","    test_evaluation_scores = [calculate_precision_recall_fscore(test_tags[i], test_keywords_and_scores[i]) for i in range(len(test_tags))]\n","    test_match_keywords, test_match_percent = matches_keywords(test_keywords_and_scores, test_tags)\n","    test_dataframe_lda = pd.DataFrame(zip(test_sentences, test_tags, test_keywords_and_scores, test_match_keywords, test_match_percent), columns=['sentences', 'tags', 'keywords', 'match', 'match percent'])\n","    test_dataframe_lda['precision'], test_dataframe_lda['recall'], test_dataframe_lda['f_score'] = zip(*test_evaluation_scores)\n","    test_dataframe_lda.to_csv(\"../results/csv/Wikipedia_lda-keywords-match-test_df.csv\", index=False)\n","\n","    print(test_dataframe_lda[['sentences', 'tags', 'keywords', 'match', 'match percent', 'precision', 'recall', 'f_score']].head())"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
